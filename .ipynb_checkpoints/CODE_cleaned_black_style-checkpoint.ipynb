{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92364121",
   "metadata": {},
   "source": [
    "<center><h2> Subsurface acoustics from biogeochemical floats as a pathway to scalable autonomous observations of global surface wind </h2></center>\n",
    "    \n",
    "<center><h3> Analysis and figures </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98f4d7",
   "metadata": {},
   "source": [
    "**L. Delaigue<sup>1</sup>\\, P. Cauchy<sup>2</sup>, D. Cazau<sup>3</sup>, R. Bozzano<sup>4</sup>, S. Pensieri<sup>4</sup>, A. Gros-Martial<sup>5</sup>, J. Bonnel<sup>6</sup>, E. Leymarie<sup>1</sup> and H. Claustre<sup>1</sup>**\n",
    "\n",
    "<sup>1</sup>Sorbonne Université, CNRS, Laboratoire d'Océanographie de Villefranche, LOV, 06230 Villefranche-sur-Mer, France\n",
    "\n",
    "<sup>2</sup>Institut des sciences de la mer (ISMER), Université du Québec à Rimouski (UQAR), Rimouski, Canada\n",
    "\n",
    "<sup>3</sup>ENSTA, Lab-STICC, UMR CNRS 6285, Brest, France\n",
    "\n",
    "<sup>4</sup>Institute for the Study of Anthropic Impact and Sustainability in the Marine Environment (IAS), Consiglio Nazionale delle Ricerche (CNR), Genoa, Italy\n",
    "\n",
    "<sup>5</sup>Centre d’Études Biologiques de Chizé, CNRS, Villiers-en-bois, France\n",
    "\n",
    "<sup>6</sup>Marine Physical Laboratory, Scripps Institution of Oceanography, University of California San Diego, La Jolla, CA, 92093, USA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bb027",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c6d88",
   "metadata": {},
   "source": [
    "Wind forcing plays a pivotal role in driving upper-ocean physical and biogeochemical processes, yet direct wind observations remain sparse in many regions of the global ocean. While passive acoustic techniques have been used to estimate wind speed from moored and mobile platforms, their application to profiling floats has been demonstrated only in limited cases and remains largely unexplored. Here, we report on the first deployment of a Biogeochemical-Argo (BGC-Argo) float equipped with a passive acoustic sensor, aimed at detecting wind-driven surface signals from depth. The float was deployed in the northwestern Mediterranean Sea near the DYFAMED meteorological buoy from February to April 2025, operating at parking depths of 500–1000 m. We demonstrate that wind speed can be successfully retrieved from subsurface ambient noise using established acoustic algorithms, with float-derived estimates showing good agreement with collocated surface observations from the DYFAMED buoy. To evaluate the potential for broader application, we simulate a remote deployment scenario by refitting the acoustic model of Nystuen et al. (2015) using ERA5 reanalysis as a proxy for surface wind. Refitting the model to ERA5 data demonstrates that the float–acoustic–wind relationship is generalizable in moderate conditions, but high-wind regimes remain systematically biased—especially above 10 m s-1. Finally, we apply a residual learning framework to correct these estimates using a limited subset of DYFAMED wind data, simulating conditions where only brief surface observations—such as those from a ship during float deployment—are available. The corrected wind time series achieved a 37% reduction in RMSE and improved the coefficient of determination (R2) from 0.85 to 0.91, demonstrating the effectiveness of combining reanalysis with sparse in situ fitting. This framework enables the retrieval of fine-scale wind variability not captured by reanalysis alone, supporting a scalable strategy for float-based wind monitoring in data-sparse ocean regions—with important implications for quantifying air–sea exchanges, improving biogeochemical flux estimates, and advancing global climate observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a8013",
   "metadata": {},
   "source": [
    "## What to expect from this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b06c1",
   "metadata": {},
   "source": [
    "#### This notebook provides a complete walkthrough of the data processing and analysis workflows used in this study. It includes code for preprocessing float and reference datasets, computing acoustic metrics, applying and refitting empirical wind retrieval models, evaluating performance against in situ and reanalysis wind products, and generating all associated figures and metrics reported in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8e001",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47929a7a",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d866d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from datetime import timedelta\n",
    "from geopy.distance import geodesic\n",
    "from haversine import haversine, Unit\n",
    "from pyproj import Geod\n",
    "from scipy import stats\n",
    "from scipy.integrate import cumulative_trapezoid as cumtrapz\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import interp1d, PchipInterpolator\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import linregress\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmocean as cm\n",
    "import glob\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- === standard library === ---\n",
    "# --- === core scientific libraries === ---\n",
    "# --- === scientific tools === ---\n",
    "# --- === visualization === ---\n",
    "# --- === progress bar === ---\n",
    "# --- === geospatial distance === ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955eb4f",
   "metadata": {},
   "source": [
    "## 1 - Process raw float data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the path pattern for all matching files ---\n",
    "file_pattern = \"data/float/csv/lovuse024c_*.csv\" # Adjust the folder path if needed\n",
    "# --- Find all matching csv files ---\n",
    "csv_files = glob.glob(file_pattern)\n",
    "# --- Define coordinate mapping (station_number -> (latitude, longitude)) ---\n",
    "coordinate_map = {\n",
    "# --- First deployment ---\n",
    " \"040\": (43.698656, 7.308651),\n",
    " \"041\": (43.395203, 7.867331),\n",
    " \"042\": (43.385816, 7.835610),\n",
    " \"043\": (43.383342, 7.805394),\n",
    " \"044\": (43.374554, 7.780598),\n",
    " \"045\": (43.347018, 7.761803),\n",
    " \"046\": (43.318718, 7.743787),\n",
    " \"047\": (43.306527, 7.715931),\n",
    " \"048\": (43.293565, 7.683848),\n",
    " \"049\": (43.271333, 7.634016),\n",
    " \"050\": (43.250653, 7.578438),\n",
    " \"051\": (43.237131, 7.537831),\n",
    " \"052\": (43.221096, 7.488428),\n",
    " \"053\": (43.181266, 7.432160),\n",
    " \"054\": (43.143442, 7.408066),\n",
    " \"055\": (43.113340, 7.377611),\n",
    " \"056\": (43.094762, 7.322352),\n",
    " \"057\": (43.080311, 7.263309),\n",
    "# --- Second deployment ---\n",
    " \"058\": (43.417590, 7.798665),\n",
    " \"059\": (43.387746, 7.777208),\n",
    " \"060\": (43.363777, 7.730860),\n",
    " \"061\": (43.342675, 7.640585),\n",
    " \"062\": (43.313547, 7.565492),\n",
    " \"063\": (43.266750, 7.461486),\n",
    " \"064\": (43.240656, 7.400188),\n",
    " \"065\": (43.222551, 7.300435),\n",
    " \"066\": (43.208608, 7.130404),\n",
    " \"067\": (43.173637, 6.977106),\n",
    " \"068\": (43.167145, 6.954846),\n",
    " \"069\": (43.173564, 6.957003)\n",
    "}\n",
    "# --- List to store dataframes ---\n",
    "float_data_list = []\n",
    "# --- Process each file ---\n",
    "for file in csv_files:\n",
    "# --- Extract station number from filename ---\n",
    " match = re.search(r'lovuse024c_(\\d{3})_', file)\n",
    " if match:\n",
    " station_number = match.group(1) # Extract station number\n",
    " if station_number in coordinate_map:\n",
    " lat, lon = coordinate_map[station_number]\n",
    "# --- Load csv file ---\n",
    " df = pd.read_csv(file, sep=\",\")\n",
    "# --- Add latitude, longitude, and station number columns ---\n",
    " df[\"Latitude\"] = lat\n",
    " df[\"Longitude\"] = lon\n",
    " df[\"StationNumber\"] = station_number # Store station number\n",
    "# --- Store in list ---\n",
    " float_data_list.append(df)\n",
    "# --- Combine all files into one dataframe ---\n",
    "float_data = pd.concat(float_data_list, ignore_index = True)\n",
    "# --- Convert date column to datetime ---\n",
    "float_data[\"Date\"] = pd.to_datetime(float_data[\"Date\"])\n",
    "# --- Separate data based on sensortype ---\n",
    "sbe41_data = float_data[float_data[\"SensorType\"] == \"sbe41\"].copy()\n",
    "pal_data = float_data[float_data[\"SensorType\"] == \"pal\"].copy()\n",
    "do_data = float_data[float_data[\"SensorType\"] == \"do\"].copy()\n",
    "# --- Convert date to matplotlib float format for kdtree ---\n",
    "sbe41_data[\"datenum\"] = sbe41_data[\"Date\"].map(mdates.date2num)\n",
    "pal_data[\"datenum\"] = pal_data[\"Date\"].map(mdates.date2num)\n",
    "do_data[\"datenum\"] = do_data[\"Date\"].map(mdates.date2num)\n",
    "# --- Ensure pressure is numeric ---\n",
    "sbe41_data[\"Pressure_dbar\"] = pd.to_numeric(sbe41_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "pal_data[\"Pressure_dbar\"] = pd.to_numeric(pal_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "do_data[\"Pressure_dbar\"] = pd.to_numeric(do_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "# --- Do remains the base dataset (7328 rows) ---\n",
    "final_data = do_data.copy()\n",
    "# --- Prepare empty lists for the new matched columns ---\n",
    "sbe41_matched_cols = {col: [] for col in sbe41_data.columns if col not in [\"datenum\", \"Pressure_dbar\", \"PhaseName\", \"SensorType\"]}\n",
    "pal_matched_cols = {col: [] for col in pal_data.columns if col not in [\"datenum\", \"Pressure_dbar\", \"PhaseName\", \"SensorType\"]}\n",
    "# --- Normalization by bandwidth to get spl in db re 1 µpa²/hz from abysens ---\n",
    "# --- Coeff = 10* log10 ( freq_max - freq_min ) ---\n",
    "# --- Freq_max = fc*2 ** (1. / 6) ---\n",
    "# --- Freq_min = fc*2 ** (-1. / 6) ---\n",
    "# --- Avec fc la fréquence centrale de la bande ---\n",
    "# --- So the frequency_correction is really the normaliwation to spl in db re 1 µpa²/hz ---\n",
    "frequency_correction = {\n",
    " \"f_25000Hz\": -37.69, \"f_20000Hz\": -36.69, \"f_16000Hz\": -35.69, \"f_12500Hz\": -34.68,\n",
    " \"f_10000Hz\": -33.68, \"f_8000Hz\": -32.68, \"f_6300Hz\": -31.67, \"f_5000Hz\": -30.67,\n",
    " \"f_4000Hz\": -29.67, \"f_3150Hz\": -28.66, \"f_2500Hz\": -27.66, \"f_2000Hz\": -26.66,\n",
    " \"f_1600Hz\": -25.65, \"f_1250Hz\": -24.65, \"f_1000Hz\": -23.65, \"f_800Hz\": -22.64,\n",
    " \"f_630Hz\": -21.64, \"f_500Hz\": -20.64, \"f_400Hz\": -19.63, \"f_160Hz\": -15.62,\n",
    " \"f_125Hz\": -14.62, \"f_100Hz\": -13.61, \"f_63Hz\": -11.61\n",
    "}\n",
    "# --- Initialize a list to store matched pal timestamps ---\n",
    "matched_pal_dates = []\n",
    "# --- Iterate over each row in do and find the nearest match in sbe41 & pal ---\n",
    "print(\"Matching SBE41 and PAL data to DO dataset with 3-hour rolling mean...\")\n",
    "for _, do_row in tqdm(do_data.iterrows(), total = len(do_data)):\n",
    " phase = do_row[\"PhaseName\"]\n",
    " date_pressure = np.array([[do_row[\"datenum\"], do_row[\"Pressure_dbar\"]]])\n",
    "# --- Find nearest sbe41 match (same phasename + nearest date + nearest pressure) ---\n",
    " if not sbe41_data.empty and phase in sbe41_data[\"PhaseName\"].unique():\n",
    " sbe41_phase = sbe41_data[sbe41_data[\"PhaseName\"] == phase]\n",
    " if not sbe41_phase.empty:\n",
    " tree_sbe41 = cKDTree(sbe41_phase[[\"datenum\", \"Pressure_dbar\"]].values)\n",
    " _, idx_sbe41 = tree_sbe41.query(date_pressure)\n",
    " matched_sbe41 = sbe41_phase.iloc[idx_sbe41[0]]\n",
    " else:\n",
    " matched_sbe41 = pd.Series(index = sbe41_data.columns, dtype=\"float64\") # Empty row\n",
    " else:\n",
    " matched_sbe41 = pd.Series(index = sbe41_data.columns, dtype=\"float64\") # Empty row\n",
    "# --- Find nearest pal match (same phasename + nearest date + nearest pressure) ---\n",
    " if not pal_data.empty and phase in pal_data[\"PhaseName\"].unique():\n",
    " pal_phase = pal_data[pal_data[\"PhaseName\"] == phase]\n",
    " if not pal_phase.empty:\n",
    " tree_pal = cKDTree(pal_phase[[\"datenum\", \"Pressure_dbar\"]].values)\n",
    " _, idx_pal = tree_pal.query(date_pressure)\n",
    " matched_pal = pal_phase.iloc[idx_pal[0]] # Initial match\n",
    "# --- Store the timestamp before averaging ---\n",
    " matched_pal_dates.append(matched_pal[\"Date\"])\n",
    "# --- Define the rolling window: ±1.5 hours ---\n",
    " time_window_start = matched_pal[\"Date\"] - timedelta(hours = 1.5)\n",
    " time_window_end = matched_pal[\"Date\"] + timedelta(hours = 1.5)\n",
    "# --- Select pal rows within this time window ---\n",
    " pal_subset = pal_phase[\n",
    " (pal_phase[\"Date\"] >= time_window_start) &\n",
    " (pal_phase[\"Date\"] <= time_window_end)\n",
    " ].copy()\n",
    "# --- Identify all f_* columns dynamically ---\n",
    " f_columns = [col for col in pal_subset.columns if col.startswith(\"f_\")]\n",
    "# --- Apply correction for each f_* column ---\n",
    " for f_col in f_columns:\n",
    " if f_col in frequency_correction:\n",
    " correction_value = frequency_correction[f_col]\n",
    " pal_subset[f_col] += correction_value\n",
    " else:\n",
    " print(f\"Warning: No correction found for column {f_col}\")\n",
    "# --- Flag pal values above the 95th percentile before averaging ---\n",
    " for f_col in f_columns:\n",
    " threshold = pal_subset[f_col].quantile(0.95)\n",
    " pal_subset.loc[pal_subset[f_col] > threshold, f_col] = np.nan\n",
    "# --- Compute mean for numeric columns after applying correction ---\n",
    " if not pal_subset.empty:\n",
    " matched_pal = pal_subset.mean(numeric_only = True)\n",
    " else:\n",
    " matched_pal = pd.Series(index = pal_data.columns, dtype=\"float64\") # Empty row\n",
    " matched_pal_dates.append(np.nan) # Ensure we store NaN if no match found\n",
    " else:\n",
    " matched_pal = pd.Series(index = pal_data.columns, dtype=\"float64\") # Empty row\n",
    " matched_pal_dates.append(np.nan) # Store NaN for missing timestamps\n",
    "# --- Append matched data ---\n",
    " for col in sbe41_matched_cols:\n",
    " sbe41_matched_cols[col].append(matched_sbe41.get(col, np.nan))\n",
    " for col in pal_matched_cols:\n",
    " pal_matched_cols[col].append(matched_pal.get(col, np.nan)) # Store rolling mean\n",
    "# --- Add matched sbe41 and pal columns to the final dataframe ---\n",
    "for col, values in sbe41_matched_cols.items():\n",
    " final_data[col + \"_sbe41\"] = values\n",
    "for col, values in pal_matched_cols.items():\n",
    " final_data[col + \"_pal\"] = values\n",
    "# --- Add stored pal timestamps ---\n",
    "final_data[\"datetime_pal\"] = matched_pal_dates\n",
    "# --- Convert datetime_pal to datetime format ---\n",
    "final_data[\"datetime_pal\"] = pd.to_datetime(final_data[\"datetime_pal\"], errors=\"coerce\")\n",
    "# --- Final check: ensure do row count remains the same as base ---\n",
    "assert len(final_data) == len(do_data), f\"Error: Final dataset has {len(final_data)} rows instead of {len(do_data)}!\"\n",
    "# --- Identify all columns that start with \"f_\" and end with \"_pal\" ---\n",
    "pal_f_columns = [col for col in final_data.columns if col.startswith(\"f_\") and col.endswith(\"_pal\")]\n",
    "# --- Define the list of other columns to keep ---\n",
    "cols_to_keep = [\n",
    " \"StationNumber\", # Include station number\n",
    " \"Latitude\",\n",
    " \"Longitude\",\n",
    " \"Pressure_dbar\",\n",
    " \"Date\",\n",
    " \"PhaseName\",\n",
    " \"doxy_uncalibrated\",\n",
    " \"datenum\",\n",
    " \"Date_sbe41\",\n",
    " \"Temperature_degC_sbe41\",\n",
    " \"Salinity_PSU_sbe41\",\n",
    " \"datetime_pal\",\n",
    "] + pal_f_columns # Append dynamically identified PAL columns\n",
    "# --- Keep only the selected columns ---\n",
    "final_data = final_data[cols_to_keep]\n",
    "# --- Update renaming dictionary ---\n",
    "rename_dict = {\n",
    " \"StationNumber\": \"station_number\", # Rename station number\n",
    " \"Latitude\":\"latitude\",\n",
    " \"Longitude\":\"longitude\",\n",
    " \"Pressure_dbar\": \"pressure\",\n",
    " \"Date\": \"datetime_DO\",\n",
    " \"PhaseName\": \"phasename\",\n",
    " \"doxy_uncalibrated\": \"doxy\",\n",
    " \"datenum\": \"datenum\",\n",
    " \"Date_sbe41\": \"datetime_SBE41\",\n",
    " \"Temperature_degC_sbe41\": \"temperature\",\n",
    " \"Salinity_PSU_sbe41\": \"salinity\",\n",
    "}\n",
    "# --- Add dynamically identified pal renaming ---\n",
    "rename_dict.update({col: col.replace(\"_pal\", \"\") for col in pal_f_columns})\n",
    "float_data = final_data.rename(columns = rename_dict)\n",
    "del final_data\n",
    "# --- Converting 'datetime_do' to datetime format if it's not already ---\n",
    "float_data['datetime_DO'] = pd.to_datetime(float_data['datetime_DO'], errors='coerce')\n",
    "# --- Final_data new columns for year, month, and day ---\n",
    "float_data['year'] = float_data['datetime_DO'].dt.year\n",
    "float_data['month'] = float_data['datetime_DO'].dt.month\n",
    "float_data['day'] = float_data['datetime_DO'].dt.day\n",
    "# --- Convert datetime to numerical format for plotting later ---\n",
    "float_data[\"datenum\"] = mdates.date2num(float_data[\"datetime_DO\"])\n",
    "# --- Ensuite station number remains ---\n",
    "float_data[\"station_number\"] = float_data[\"station_number\"].astype(str) # Ensure it's treated as a string\n",
    "# --- Save final merged dataset ---\n",
    "float_data.to_csv(\"data/sea_trials_do_sbe41_pal.csv\", index = False)\n",
    "print(\"Merged data saved successfully. SPL values in dB re 1 µPa²/Hz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190248b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the path pattern for all matching files ---\n",
    "file_pattern = \"data/float/csv/lovuse024c_*.csv\" # Adjust the folder path if needed\n",
    "# --- Find all matching csv files ---\n",
    "csv_files = glob.glob(file_pattern)\n",
    "# --- Define coordinate mapping (station_number -> (latitude, longitude)) ---\n",
    "coordinate_map = {\n",
    "# --- First deployment ---\n",
    " \"040\": (43.698656, 7.308651),\n",
    " \"041\": (43.395203, 7.867331),\n",
    " \"042\": (43.385816, 7.835610),\n",
    " \"043\": (43.383342, 7.805394),\n",
    " \"044\": (43.374554, 7.780598),\n",
    " \"045\": (43.347018, 7.761803),\n",
    " \"046\": (43.318718, 7.743787),\n",
    " \"047\": (43.306527, 7.715931),\n",
    " \"048\": (43.293565, 7.683848),\n",
    " \"049\": (43.271333, 7.634016),\n",
    " \"050\": (43.250653, 7.578438),\n",
    " \"051\": (43.237131, 7.537831),\n",
    " \"052\": (43.221096, 7.488428),\n",
    " \"053\": (43.181266, 7.432160),\n",
    " \"054\": (43.143442, 7.408066),\n",
    " \"055\": (43.113340, 7.377611),\n",
    " \"056\": (43.094762, 7.322352),\n",
    " \"057\": (43.080311, 7.263309),\n",
    "# --- Second deployment ---\n",
    " \"058\": (43.417590, 7.798665),\n",
    " \"059\": (43.387746, 7.777208),\n",
    " \"060\": (43.363777, 7.730860),\n",
    " \"061\": (43.342675, 7.640585),\n",
    " \"062\": (43.313547, 7.565492),\n",
    " \"063\": (43.266750, 7.461486),\n",
    " \"064\": (43.240656, 7.400188),\n",
    " \"065\": (43.222551, 7.300435),\n",
    " \"066\": (43.208608, 7.130404),\n",
    " \"067\": (43.173637, 6.977106),\n",
    " \"068\": (43.167145, 6.954846),\n",
    " \"069\": (43.173564, 6.957003)\n",
    "}\n",
    "# --- List to store dataframes ---\n",
    "float_data_list = []\n",
    "# --- Process each file ---\n",
    "for file in csv_files:\n",
    "# --- Extract station number from filename ---\n",
    " match = re.search(r'lovuse024c_(\\d{3})_', file)\n",
    " if match:\n",
    " station_number = match.group(1) # Extract station number\n",
    " if station_number in coordinate_map:\n",
    " lat, lon = coordinate_map[station_number]\n",
    "# --- Load csv file ---\n",
    " df = pd.read_csv(file, sep=\",\")\n",
    "# --- Add latitude, longitude, and station number columns ---\n",
    " df[\"Latitude\"] = lat\n",
    " df[\"Longitude\"] = lon\n",
    " df[\"StationNumber\"] = station_number # Store station number\n",
    "# --- Store in list ---\n",
    " float_data_list.append(df)\n",
    "# --- Combine all files into one dataframe ---\n",
    "float_data = pd.concat(float_data_list, ignore_index = True)\n",
    "# --- Convert date column to datetime ---\n",
    "float_data[\"Date\"] = pd.to_datetime(float_data[\"Date\"])\n",
    "# --- Separate data based on sensortype ---\n",
    "sbe41_data = float_data[float_data[\"SensorType\"] == \"sbe41\"].copy()\n",
    "pal_data = float_data[float_data[\"SensorType\"] == \"pal\"].copy()\n",
    "do_data = float_data[float_data[\"SensorType\"] == \"do\"].copy()\n",
    "# --- Convert date to matplotlib float format for kdtree ---\n",
    "sbe41_data[\"datenum\"] = sbe41_data[\"Date\"].map(mdates.date2num)\n",
    "pal_data[\"datenum\"] = pal_data[\"Date\"].map(mdates.date2num)\n",
    "do_data[\"datenum\"] = do_data[\"Date\"].map(mdates.date2num)\n",
    "# --- Ensure pressure is numeric ---\n",
    "sbe41_data[\"Pressure_dbar\"] = pd.to_numeric(sbe41_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "pal_data[\"Pressure_dbar\"] = pd.to_numeric(pal_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "do_data[\"Pressure_dbar\"] = pd.to_numeric(do_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "# --- Do remains the base dataset (7328 rows) ---\n",
    "final_data = do_data.copy()\n",
    "# --- Prepare empty lists for the new matched columns ---\n",
    "sbe41_matched_cols = {col: [] for col in sbe41_data.columns if col not in [\"datenum\", \"Pressure_dbar\", \"PhaseName\", \"SensorType\"]}\n",
    "# --- Only keep f_3150hz and f_8000hz in pal matching ---\n",
    "pal_f_keep = [\"f_3150Hz\", \"f_8000Hz\"]\n",
    "pal_matched_cols = {col: [] for col in pal_data.columns if col in pal_f_keep or col in [\"Date\"]}\n",
    "# --- Normalization by bandwidth to get spl in db re 1 µpa²/hz from abysens ---\n",
    "frequency_correction = {\n",
    " \"f_3150Hz\": -28.66,\n",
    " \"f_8000Hz\": -32.68,\n",
    "}\n",
    "# --- Initialize a list to store matched pal timestamps ---\n",
    "matched_pal_dates = []\n",
    "# --- Iterate over each row in do and find the nearest match in sbe41 & pal ---\n",
    "print(\"Matching SBE41 and PAL data to DO dataset with 3-hour rolling mean...\")\n",
    "for _, do_row in tqdm(do_data.iterrows(), total = len(do_data)):\n",
    " phase = do_row[\"PhaseName\"]\n",
    " date_pressure = np.array([[do_row[\"datenum\"], do_row[\"Pressure_dbar\"]]])\n",
    "# --- Find nearest sbe41 match (same phasename + nearest date + nearest pressure) ---\n",
    " if not sbe41_data.empty and phase in sbe41_data[\"PhaseName\"].unique():\n",
    " sbe41_phase = sbe41_data[sbe41_data[\"PhaseName\"] == phase]\n",
    " if not sbe41_phase.empty:\n",
    " tree_sbe41 = cKDTree(sbe41_phase[[\"datenum\", \"Pressure_dbar\"]].values)\n",
    " _, idx_sbe41 = tree_sbe41.query(date_pressure)\n",
    " matched_sbe41 = sbe41_phase.iloc[idx_sbe41[0]]\n",
    " else:\n",
    " matched_sbe41 = pd.Series(index = sbe41_data.columns, dtype=\"float64\") # Empty row\n",
    " else:\n",
    " matched_sbe41 = pd.Series(index = sbe41_data.columns, dtype=\"float64\") # Empty row\n",
    "# --- Find nearest pal match (same phasename + nearest date + nearest pressure) ---\n",
    " if not pal_data.empty and phase in pal_data[\"PhaseName\"].unique():\n",
    " pal_phase = pal_data[pal_data[\"PhaseName\"] == phase]\n",
    " if not pal_phase.empty:\n",
    " tree_pal = cKDTree(pal_phase[[\"datenum\", \"Pressure_dbar\"]].values)\n",
    " _, idx_pal = tree_pal.query(date_pressure)\n",
    " matched_pal = pal_phase.iloc[idx_pal[0]] # Initial match\n",
    "# --- Store the timestamp before averaging ---\n",
    " matched_pal_dates.append(matched_pal[\"Date\"])\n",
    "# --- Define the rolling window: ±1.5 hours ---\n",
    " time_window_start = matched_pal[\"Date\"] - timedelta(hours = 1.5)\n",
    " time_window_end = matched_pal[\"Date\"] + timedelta(hours = 1.5)\n",
    "# --- Select pal rows within this time window ---\n",
    " pal_subset = pal_phase[\n",
    " (pal_phase[\"Date\"] >= time_window_start) &\n",
    " (pal_phase[\"Date\"] <= time_window_end)\n",
    " ].copy()\n",
    "# --- Identify all f_* columns dynamically but keep only the two ---\n",
    " f_columns = [col for col in pal_subset.columns if col in pal_f_keep]\n",
    "# --- Apply no correction yet! we do this later. ---\n",
    "# --- Flag pal values above the 95th percentile before averaging ---\n",
    " for f_col in f_columns:\n",
    " threshold = pal_subset[f_col].quantile(0.95)\n",
    " pal_subset.loc[pal_subset[f_col] > threshold, f_col] = np.nan\n",
    "# --- Compute mean for numeric columns after flagging ---\n",
    " if not pal_subset.empty:\n",
    " matched_pal_mean = pal_subset.mean(numeric_only = True)\n",
    "# --- Make sure all requested keys exist (else will be nan) ---\n",
    " matched_pal = pd.Series({col: matched_pal_mean.get(col, np.nan) for col in pal_f_keep})\n",
    " matched_pal[\"Date\"] = matched_pal_dates[-1]\n",
    " else:\n",
    " matched_pal = pd.Series({col: np.nan for col in pal_f_keep + [\"Date\"]})\n",
    " else:\n",
    " matched_pal = pd.Series({col: np.nan for col in pal_f_keep + [\"Date\"]})\n",
    " matched_pal_dates.append(np.nan) # Ensure we store NaN if no match found\n",
    " else:\n",
    " matched_pal = pd.Series({col: np.nan for col in pal_f_keep + [\"Date\"]})\n",
    " matched_pal_dates.append(np.nan) # Store NaN for missing timestamps\n",
    "# --- Append matched data ---\n",
    " for col in sbe41_matched_cols:\n",
    " sbe41_matched_cols[col].append(matched_sbe41.get(col, np.nan))\n",
    " for col in pal_matched_cols:\n",
    " pal_matched_cols[col].append(matched_pal.get(col, np.nan))\n",
    "# --- Add matched sbe41 columns to the final dataframe ---\n",
    "for col, values in sbe41_matched_cols.items():\n",
    " final_data[col + \"_sbe41\"] = values\n",
    "# --- Add matched pal columns to the final dataframe ---\n",
    "for col, values in pal_matched_cols.items():\n",
    " if col == \"Date\":\n",
    " final_data[\"datetime_pal\"] = values\n",
    " else:\n",
    " final_data[col + \"_pal\"] = values\n",
    "# --- Convert datetime_pal to datetime format ---\n",
    "final_data[\"datetime_pal\"] = pd.to_datetime(final_data[\"datetime_pal\"], errors=\"coerce\")\n",
    "# --- Final check: ensure do row count remains the same as base ---\n",
    "assert len(final_data) == len(do_data), f\"Error: Final dataset has {len(final_data)} rows instead of {len(do_data)}!\"\n",
    "# --- Only keep f_3150hz_pal and f_8000hz_pal columns from pal (plus the others you specify) ---\n",
    "pal_f_columns = [\"f_3150Hz_pal\", \"f_8000Hz_pal\"]\n",
    "# --- Define the list of other columns to keep ---\n",
    "cols_to_keep = [\n",
    " \"StationNumber\", # Include station number\n",
    " \"Latitude\",\n",
    " \"Longitude\",\n",
    " \"Pressure_dbar\",\n",
    " \"Date\",\n",
    " \"PhaseName\",\n",
    " \"doxy_uncalibrated\",\n",
    " \"datenum\",\n",
    " \"Date_sbe41\",\n",
    " \"Temperature_degC_sbe41\",\n",
    " \"Salinity_PSU_sbe41\",\n",
    " \"datetime_pal\",\n",
    "] + pal_f_columns # Only the two PAL f_ columns\n",
    "# --- Keep only the selected columns ---\n",
    "final_data = final_data[cols_to_keep]\n",
    "# --- Update renaming dictionary ---\n",
    "rename_dict = {\n",
    " \"StationNumber\": \"station_number\", # Rename station number\n",
    " \"Latitude\":\"latitude\",\n",
    " \"Longitude\":\"longitude\",\n",
    " \"Pressure_dbar\": \"pressure\",\n",
    " \"Date\": \"datetime_DO\",\n",
    " \"PhaseName\": \"phasename\",\n",
    " \"doxy_uncalibrated\": \"doxy\",\n",
    " \"datenum\": \"datenum\",\n",
    " \"Date_sbe41\": \"datetime_SBE41\",\n",
    " \"Temperature_degC_sbe41\": \"temperature\",\n",
    " \"Salinity_PSU_sbe41\": \"salinity\",\n",
    " \"f_3150Hz_pal\": \"f_3150Hz\",\n",
    " \"f_8000Hz_pal\": \"f_8000Hz\",\n",
    "}\n",
    "float_data = final_data.rename(columns = rename_dict)\n",
    "del final_data\n",
    "# --- Converting 'datetime_do' to datetime format if it's not already ---\n",
    "float_data['datetime_DO'] = pd.to_datetime(float_data['datetime_DO'], errors='coerce')\n",
    "# --- Final_data new columns for year, month, and day ---\n",
    "float_data['year'] = float_data['datetime_DO'].dt.year\n",
    "float_data['month'] = float_data['datetime_DO'].dt.month\n",
    "float_data['day'] = float_data['datetime_DO'].dt.day\n",
    "# --- Convert datetime to numerical format for plotting later ---\n",
    "float_data[\"datenum\"] = mdates.date2num(float_data[\"datetime_DO\"])\n",
    "# --- Ensuite station number remains ---\n",
    "float_data[\"station_number\"] = float_data[\"station_number\"].astype(str) # Ensure it's treated as a string\n",
    "# --- Begin beta/depth correction + freq correction ---\n",
    "# --- Load beta values from matlab run ---\n",
    "beta_df = pd.read_csv(\"data/beta_profile_output.csv\")\n",
    "# --- Convert complex strings to python complex numbers and extract real part ---\n",
    "beta_df['Beta_dB_f3150'] = beta_df['Beta_dB_f3150'].apply(lambda x: complex(str(x).replace('i', 'j'))).apply(np.real)\n",
    "beta_df['Beta_dB_f8000'] = beta_df['Beta_dB_f8000'].apply(lambda x: complex(str(x).replace('i', 'j'))).apply(np.real)\n",
    "# --- Interpolate/extrapolate to 0 m using first two points ---\n",
    "d1, d2 = beta_df['Depth_m'].iloc[0], beta_df['Depth_m'].iloc[1]\n",
    "b1_3150, b2_3150 = beta_df['Beta_dB_f3150'].iloc[0], beta_df['Beta_dB_f3150'].iloc[1]\n",
    "b1_8000, b2_8000 = beta_df['Beta_dB_f8000'].iloc[0], beta_df['Beta_dB_f8000'].iloc[1]\n",
    "# --- Linear extrapolation to 0 m ---\n",
    "beta_0_3150 = b1_3150 + (b1_3150 - b2_3150) / (d1 - d2) * (0 - d1)\n",
    "beta_0_8000 = b1_8000 + (b1_8000 - b2_8000) / (d1 - d2) * (0 - d1)\n",
    "# --- Insert new row at the top ---\n",
    "beta_df_extended = pd.concat([\n",
    " pd.DataFrame({\n",
    " 'Depth_m': [0],\n",
    " 'Beta_dB_f3150': [beta_0_3150],\n",
    " 'Beta_dB_f8000': [beta_0_8000]\n",
    " }),\n",
    " beta_df\n",
    "], ignore_index = True)\n",
    "# --- Plot using fig, ax ---\n",
    "fig, ax = plt.subplots(figsize=(6, 5), dpi = 300)\n",
    "ax.plot(beta_df_extended['Beta_dB_f3150'], beta_df_extended['Depth_m'], c='k', label='3150 Hz')\n",
    "ax.plot(beta_df_extended['Beta_dB_f8000'], beta_df_extended['Depth_m'], c='k', linestyle='--', label='8000 Hz')\n",
    "# --- Customize plot ---\n",
    "ax.set_xlabel('β (dB)')\n",
    "ax.set_ylabel('Depth (m)')\n",
    "ax.set_xlim(0, 5)\n",
    "ax.set_ylim(0, 1000)\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha = 0.3)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figs/beta_profile.png\", dpi = 300, bbox_inches='tight')\n",
    "plt.show()\n",
    "# --- Create interpolation functions (with extrapolation enabled) ---\n",
    "interp_3150 = interp1d(beta_df[\"Depth_m\"], beta_df[\"Beta_dB_f3150\"], bounds_error = False, fill_value=\"extrapolate\")\n",
    "interp_8000 = interp1d(beta_df[\"Depth_m\"], beta_df[\"Beta_dB_f8000\"], bounds_error = False, fill_value=\"extrapolate\")\n",
    "# --- Interpolate beta for each row based on pressure ---\n",
    "float_data[\"beta_f3150\"] = interp_3150(float_data[\"pressure\"])\n",
    "float_data[\"beta_f8000\"] = interp_8000(float_data[\"pressure\"])\n",
    "# --- Apply depth correction (i.e., add beta) ---\n",
    "float_data[\"f_3150Hz_beta\"] = float_data[\"f_3150Hz\"] + float_data[\"beta_f3150\"]\n",
    "float_data[\"f_8000Hz_beta\"] = float_data[\"f_8000Hz\"] + float_data[\"beta_f8000\"]\n",
    "# --- Now apply frequency correction (to the depth-corrected values) ---\n",
    "float_data[\"f_3150Hz_corrected\"] = float_data[\"f_3150Hz_beta\"] + frequency_correction[\"f_3150Hz\"]\n",
    "float_data[\"f_8000Hz_corrected\"] = float_data[\"f_8000Hz_beta\"] + frequency_correction[\"f_8000Hz\"]\n",
    "# --- Save final merged dataset ---\n",
    "float_data.to_csv(\"data/sea_trials_do_sbe41_pal.csv\", index = False)\n",
    "print(\"Merged data saved successfully. SPL values in dB re 1 µPa²/Hz with depth and frequency corrections applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb52bf8",
   "metadata": {},
   "source": [
    "## 3 - Plot deployment trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00121321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- === copy input data === ---\n",
    "df = float_data.copy()\n",
    "# --- === prepare station data === ---\n",
    "stations_df = (\n",
    " df[[\"station_number\", \"latitude\", \"longitude\", \"datetime_DO\"]]\n",
    " .drop_duplicates(subset=[\"station_number\", \"latitude\", \"longitude\"])\n",
    " .sort_values(\"station_number\")\n",
    " .reset_index(drop = True)\n",
    ")\n",
    "stations_df[\"datetime_DO\"] = pd.to_datetime(stations_df[\"datetime_DO\"])\n",
    "stations_df = stations_df.sort_values(\"datetime_DO\").reset_index(drop = True)\n",
    "# --- === interpolate trajectory === ---\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "interpolated_rows = []\n",
    "for i in range(len(stations_df) - 1):\n",
    " start = stations_df.iloc[i]\n",
    " end = stations_df.iloc[i + 1]\n",
    " start_time = start[\"datetime_DO\"]\n",
    " end_time = end[\"datetime_DO\"]\n",
    " duration_hours = int((end_time - start_time).total_seconds() // 3600)\n",
    " if duration_hours <= 0:\n",
    " continue\n",
    "# --- Intermediate points between stations ---\n",
    " lonlats = geod.npts(\n",
    " start[\"longitude\"], start[\"latitude\"],\n",
    " end[\"longitude\"], end[\"latitude\"],\n",
    " duration_hours - 1\n",
    " )\n",
    "# --- Add start point ---\n",
    " interpolated_rows.append({\n",
    " \"datetime\": start_time,\n",
    " \"latitude\": start[\"latitude\"],\n",
    " \"longitude\": start[\"longitude\"]\n",
    " })\n",
    "# --- Add interpolated points ---\n",
    " for j, (lon, lat) in enumerate(lonlats):\n",
    " interpolated_rows.append({\n",
    " \"datetime\": start_time + timedelta(hours = j + 1),\n",
    " \"latitude\": lat,\n",
    " \"longitude\": lon\n",
    " })\n",
    "# --- Add final point ---\n",
    "interpolated_rows.append({\n",
    " \"datetime\": stations_df.iloc[-1][\"datetime_DO\"],\n",
    " \"latitude\": stations_df.iloc[-1][\"latitude\"],\n",
    " \"longitude\": stations_df.iloc[-1][\"longitude\"]\n",
    "})\n",
    "trajectory_df = pd.DataFrame(interpolated_rows)\n",
    "trajectory_df[\"datetime\"] = pd.to_datetime(trajectory_df[\"datetime\"])\n",
    "# --- === filter out unwanted time window === ---\n",
    "trajectory_df = trajectory_df[\n",
    " ~trajectory_df[\"datetime\"].between(\"2025-03-10 08:00:00\", \"2025-03-13 06:00:00\")\n",
    "].reset_index(drop = True)\n",
    "# --- === load bathymetry data === ---\n",
    "bathymetry = xr.open_dataset(\n",
    " \"data/GEBCO_21_Mar_2025_ecf26466cdc0/GEBCO_21_Mar_2025_ecf26466cdc0/gebco_2024_n44.881_s41.153_w4.3652_e9.7485.nc\"\n",
    ")\n",
    "# --- === split deployments based on station 058 === ---\n",
    "deployment_boundary = float_data[float_data[\"station_number\"] == \"058\"][\"datenum\"].min()\n",
    "deployment_boundary_dt = mdates.num2date(deployment_boundary).replace(tzinfo = None)\n",
    "deployment_1 = trajectory_df[trajectory_df[\"datetime\"] < deployment_boundary_dt]\n",
    "deployment_2 = trajectory_df[trajectory_df[\"datetime\"] >= deployment_boundary_dt]\n",
    "# --- === plot setup === ---\n",
    "fig = plt.figure(dpi = 300, figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection = ccrs.PlateCarree())\n",
    "# --- Base map ---\n",
    "ax.add_feature(\n",
    " cfeature.NaturalEarthFeature(\"physical\", \"land\", \"10m\"),\n",
    " facecolor=\"xkcd:light grey\", edgecolor=\"k\"\n",
    ")\n",
    "ax.add_feature(cfeature.BORDERS, linewidth = 0.5, edgecolor=\"k\")\n",
    "# --- Bathymetry ---\n",
    "bathymetry.elevation.plot(\n",
    " add_colorbar = False,\n",
    " ax = ax,\n",
    " cmap = cm.cm.topo,\n",
    " transform = ccrs.PlateCarree(),\n",
    " zorder = 0\n",
    ")\n",
    "# --- Deployment paths ---\n",
    "ax.plot(\n",
    " deployment_1[\"longitude\"], deployment_1[\"latitude\"],\n",
    " color=\"xkcd:light red\", linestyle=\"solid\", linewidth = 2,\n",
    " transform = ccrs.PlateCarree(), label=\"Deployment A\"\n",
    ")\n",
    "ax.plot(\n",
    " deployment_2[\"longitude\"], deployment_2[\"latitude\"],\n",
    " color=\"xkcd:light red\", linestyle=\"dashed\", linewidth = 2,\n",
    " transform = ccrs.PlateCarree(), label=\"Deployment B\"\n",
    ")\n",
    "# --- Text annotations ---\n",
    "ax.plot(7.75, 43.3, marker='^', color='k', markersize = 8, transform = ccrs.PlateCarree())\n",
    "ax.text(7.78, 43.3, \"DYFAMED\", transform = ccrs.PlateCarree(), fontsize = 12, fontweight='bold')\n",
    "ax.text(6.838, 43.72, \"FRANCE\", transform = ccrs.PlateCarree(), fontsize = 12, fontweight='bold')\n",
    "ax.text(7.66, 43.87, \"ITALY\", transform = ccrs.PlateCarree(), fontsize = 12, fontweight='bold')\n",
    "ax.text(7.74, 43.42, \"START\", transform = ccrs.PlateCarree(), fontsize = 8, fontweight='bold', color='xkcd:light red')\n",
    "ax.text(6.90, 43.15, \"END\", transform = ccrs.PlateCarree(), fontsize = 8, fontweight='bold', color='xkcd:light red')\n",
    "ax.text(7.88, 43.4, \"START\", transform = ccrs.PlateCarree(), fontsize = 8, fontweight='bold', color='xkcd:light red')\n",
    "ax.text(7.21, 43.07, \"END\", transform = ccrs.PlateCarree(), fontsize = 8, fontweight='bold', color='xkcd:light red')\n",
    "# --- Concentric circle (e.g., 40 km radius) around dyfamed ---\n",
    "dyfamed_lon, dyfamed_lat = 7.75, 43.3\n",
    "radii_km = [40]\n",
    "for radius in radii_km:\n",
    " angle = np.linspace(0, 360, 361)\n",
    " circle_lon, circle_lat, _ = geod.fwd(\n",
    " np.full_like(angle, dyfamed_lon),\n",
    " np.full_like(angle, dyfamed_lat),\n",
    " angle,\n",
    " np.full_like(angle, radius * 1000) # in meters\n",
    " )\n",
    " ax.plot(circle_lon, circle_lat, linestyle=\"--\", linewidth = 1, color=\"black\", transform = ccrs.PlateCarree())\n",
    "# --- Map extent and grid ---\n",
    "ax.set_extent([6.5, 8, 43, 44], crs = ccrs.PlateCarree())\n",
    "gl = ax.gridlines(draw_labels = True, alpha = 0.3)\n",
    "gl.bottom_labels = False\n",
    "gl.right_labels = False\n",
    "# --- Legend and save ---\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"figs/trajectory.png\", dpi = 300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9fd93",
   "metadata": {},
   "source": [
    "## 4 - Process DYFAMED meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load both months' data and combine ---\n",
    "files = [\"data/meteo_france_vent/marine.202502.csv\", \"data/meteo_france_vent/marine.202503.csv\", \"data/meteo_france_vent/marine.202504.csv\"]\n",
    "wind_data = pd.concat([pd.read_csv(f, sep=\";\", na_values='mq') for f in files], ignore_index = True)\n",
    "# --- Only keep data for dyfamed ---\n",
    "wind_data = wind_data[wind_data['numer_sta'] == '6100001']\n",
    "# --- Rename columns ---\n",
    "wind_data = wind_data.rename(columns={\n",
    " \"date\": \"datetime\",\n",
    " \"ff\": \"U_DYFAMED\",\n",
    "})[[\"datetime\", \"U_DYFAMED\"]]\n",
    "# --- Function to parse datetime in \"yyyymmddhhmmss\" format ---\n",
    "def parse_datetime(date_str):\n",
    " try:\n",
    " return datetime.strptime(str(date_str), \"%Y%m%d%H%M%S\")\n",
    " except Exception as e:\n",
    " print(f\"Error parsing date: {date_str} - {e}\")\n",
    " return np.nan # Return NaN if parsing fails\n",
    "# --- Apply datetime parsing ---\n",
    "wind_data[\"datetime\"] = wind_data[\"datetime\"].apply(parse_datetime)\n",
    "# --- Drop any rows where datetime parsing failed ---\n",
    "wind_data = wind_data.dropna(subset=[\"datetime\"])\n",
    "# --- Convert datetime columns to datetime format ---\n",
    "# --- Float_data[\"date\"] = pd.to_datetime(float_data[\"date\"]) ---\n",
    "wind_data[\"datetime\"] = pd.to_datetime(wind_data[\"datetime\"])\n",
    "# --- Ensure wind_data is sorted and has unique timestamps ---\n",
    "wind_data = wind_data.sort_values(\"datetime\").drop_duplicates(subset=[\"datetime\"])\n",
    "# --- Convert datetime to numerical format for plotting ---\n",
    "wind_data[\"datenum\"] = mdates.date2num(wind_data[\"datetime\"])\n",
    "# --- Add latitude and longitude ---\n",
    "wind_data[\"lat\"] = 43.38\n",
    "wind_data[\"lon\"] = 7.83\n",
    "wind_data.to_csv(\"data/wind_data_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989b3d6",
   "metadata": {},
   "source": [
    "## 5 - Match DYFAMED wind to float data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert datetime columns to datetime format ---\n",
    "float_data = float_data.reset_index(drop = True)\n",
    "float_data[\"datetime_DO\"] = pd.to_datetime(float_data[\"datetime_DO\"])\n",
    "wind_data[\"datetime\"] = pd.to_datetime(wind_data[\"datetime\"])\n",
    "# --- Ensure wind_data is sorted and has unique timestamps ---\n",
    "wind_data = wind_data.sort_values(\"datetime\").drop_duplicates(subset=[\"datetime\"])\n",
    "# --- Match with wind data using the nearest timestamp approach ---\n",
    "matched_wind_data = wind_data.set_index(\"datetime\").reindex(float_data[\"datetime_DO\"], method=\"nearest\").reset_index()\n",
    "# --- Add matched wind speed directly to float_data ---\n",
    "float_data[\"U_DYFAMED\"] = matched_wind_data[\"U_DYFAMED\"]\n",
    "# --- Extract matched f_8000hz and wind speed (after ensuring non-null values) ---\n",
    "X_data = float_data[\"f_8000Hz\"].dropna()\n",
    "Y_data = float_data[\"U_DYFAMED\"].dropna()\n",
    "# --- Ensure both datasets align (remove unmatched indices) ---\n",
    "common_indices = X_data.index.intersection(Y_data.index)\n",
    "X_data = X_data.loc[common_indices]\n",
    "Y_data = Y_data.loc[common_indices]\n",
    "# --- Define dyfamed buoy coordinates ---\n",
    "buoy_lat, buoy_lon = wind_data[\"lat\"].unique(), wind_data[\"lon\"].unique()\n",
    "# --- Ensure 'latitude' and 'longitude' exist and do not contain nan values ---\n",
    "if \"latitude\" in float_data.columns and \"longitude\" in float_data.columns:\n",
    "# --- Remove rows with nan in latitude or longitude before applying distance function ---\n",
    " valid_data = float_data.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "# --- Apply geodesic distance calculation only for valid rows ---\n",
    " valid_data[\"distance_km\"] = valid_data.apply(\n",
    " lambda row: geodesic((row[\"latitude\"], row[\"longitude\"]), (buoy_lat, buoy_lon)).km, axis = 1\n",
    " )\n",
    "# --- Merge back with float_data (ensure nan distances remain for missing coordinates) ---\n",
    " float_data = float_data.merge(valid_data[[\"datetime_DO\", \"distance_km\"]], on=\"datetime_DO\", how=\"left\")\n",
    "else:\n",
    " print(\"Error: 'latitude' and 'longitude' columns not found in float_data.\")\n",
    "# --- Normalize distances (only if distance_km exists and is valid) ---\n",
    "if \"distance_km\" in float_data.columns:\n",
    " float_data[\"norm_distance\"] = (float_data[\"distance_km\"] - float_data[\"distance_km\"].min()) / \\\n",
    " (float_data[\"distance_km\"].max() - float_data[\"distance_km\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa18b66",
   "metadata": {},
   "source": [
    "## 7- Fit already-existing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- === wind speed model functions === ---\n",
    "def nystuen2015(SPL):\n",
    " return 0.0005 * SPL**3 - 0.0310 * SPL**2 + 0.4904 * SPL + 2.0871\n",
    "def pensieri2015(SPL):\n",
    " SPL = np.asarray(SPL)\n",
    " result = np.full_like(SPL, np.nan, dtype = np.float64)\n",
    " mask1 = (SPL >= 30) & (SPL < 38)\n",
    " mask2 = (SPL >= 38) & (SPL < 60)\n",
    " result[mask1] = 0.1458 * SPL[mask1] - 3.146\n",
    " result[mask2] = 0.044642 * SPL[mask2]**2 - 3.2917 * SPL[mask2] + 63.016\n",
    " return result\n",
    "def vagle1990(SPL):\n",
    " A = 7.38\n",
    " B = -38.70\n",
    " C = SPL - 21.69\n",
    " discriminant = B**2 - 4 * A * C\n",
    " result = np.full_like(SPL, np.nan, dtype = np.float64)\n",
    " mask = discriminant >= 0\n",
    " root_term = np.sqrt(discriminant[mask])\n",
    " exponent = (-B - root_term) / (2 * A)\n",
    " result[mask] = 10 ** exponent\n",
    " return result\n",
    "def band_integration_coeff(fc):\n",
    " freq_min = fc * 2 ** (-1/6)\n",
    " freq_max = fc * 2 ** (1/6)\n",
    " return 10 * np.log10(freq_max - freq_min)\n",
    "def cauchy2018_spl(SPL, S_off, coeff):\n",
    " SPL = np.asarray(SPL)\n",
    " TOL_equiv = SPL + coeff\n",
    " p = 10 ** (TOL_equiv / 20) - S_off\n",
    "# --- Model coefficients from paper ---\n",
    " s_low = 0.4e4\n",
    " b_low = 0.2e4\n",
    " s_high = 1.6e4\n",
    " b_high = 12.5e4\n",
    " U_low = (p + b_low) / s_low\n",
    " U_high = (p + b_high) / s_high\n",
    " return np.where(U_low > 10, U_high, U_low)\n",
    "# --- Calculate cauchy band integration coeff and s_off ---\n",
    "fc = 3150\n",
    "coeff = band_integration_coeff(fc)\n",
    "S_off = np.nanmin(10 ** (float_data[\"f_3150Hz\"]/20))\n",
    "# --- === apply models === ---\n",
    "# --- Nystuen and vagle can be applied to all valid spls ---\n",
    "float_data[\"U_nystuen_litt_coeffs\"] = nystuen2015(float_data[\"f_8000Hz_corrected\"])\n",
    "float_data[\"U_vagle_litt_coeffs\"] = vagle1990(float_data[\"f_8000Hz_corrected\"])\n",
    "# --- Pensieri: set values only in valid range [30, 60), nan elsewhere ---\n",
    "SPL_8000 = float_data[\"f_8000Hz_corrected\"].values\n",
    "U_pensieri = np.full_like(SPL_8000, np.nan, dtype = np.float64)\n",
    "mask_pensieri = (SPL_8000 >= 30) & (SPL_8000 < 60)\n",
    "U_pensieri[mask_pensieri] = pensieri2015(SPL_8000[mask_pensieri])\n",
    "float_data[\"U_pensieri_litt_coeffs\"] = U_pensieri\n",
    "# --- Cauchy: apply to all non-nan spls at 3150 hz ---\n",
    "SPL_3150 = float_data[\"f_3150Hz_corrected\"].values\n",
    "mask_cauchy = ~np.isnan(SPL_3150)\n",
    "U_cauchy = np.full_like(SPL_3150, np.nan, dtype = np.float64)\n",
    "U_cauchy[mask_cauchy] = cauchy2018_spl(SPL_3150[mask_cauchy], S_off = S_off, coeff = coeff)\n",
    "float_data[\"U_cauchy_litt_coefs\"] = U_cauchy\n",
    "# --- === plot for quick validation === ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_nystuen_litt_coeffs\"], alpha = 0.3, label=\"Nystuen\")\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_pensieri_litt_coeffs\"], alpha = 0.3, label=\"Pensieri\")\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_vagle_litt_coeffs\"], alpha = 0.3, label=\"Vagle\")\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_cauchy_litt_coefs\"], alpha = 0.3, label=\"Cauchy\")\n",
    "plt.plot(\n",
    " [float_data[\"U_DYFAMED\"].min(), float_data[\"U_DYFAMED\"].max()],\n",
    " [float_data[\"U_DYFAMED\"].min(), float_data[\"U_DYFAMED\"].max()],\n",
    " 'k--', label=\"1:1 line\"\n",
    ")\n",
    "plt.xlabel(\"Reference Wind Speed (U_DYFAMED)\")\n",
    "plt.ylabel(\"Model Wind Speed (literature coeff)\")\n",
    "plt.legend()\n",
    "plt.title(\"Litt Wind Speed Model Fits\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e8f63",
   "metadata": {},
   "source": [
    "## 8 - Optimize already-existing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82799def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- === wind speed model functions === ---\n",
    "def nystuen2015_fit(SPL, a, b, c, d):\n",
    " return a * SPL**3 + b * SPL**2 + c * SPL + d\n",
    "def pensieri2015_fit(SPL, a1, b1, a2, b2, c2):\n",
    " SPL = np.asarray(SPL)\n",
    " result = np.full_like(SPL, np.nan, dtype = np.float64)\n",
    " mask1 = (SPL >= 30) & (SPL < 38)\n",
    " mask2 = (SPL >= 38) & (SPL < 60)\n",
    " result[mask1] = a1 * SPL[mask1] + b1\n",
    " result[mask2] = a2 * SPL[mask2]**2 + b2 * SPL[mask2] + c2\n",
    " return result\n",
    "def vagle1990_fit(SPL, A, B, C0):\n",
    " SPL = np.asarray(SPL)\n",
    " C = SPL - C0\n",
    " discriminant = B**2 - 4 * A * C\n",
    " result = np.full_like(SPL, np.nan, dtype = np.float64)\n",
    " mask = discriminant >= 0\n",
    " root_term = np.sqrt(discriminant[mask])\n",
    " exponent = (-B - root_term) / (2 * A)\n",
    " result[mask] = 10 ** exponent\n",
    " return result\n",
    "def cauchy2018_fit(SPL, S_off, coeff, s_low, b_low, s_high, b_high):\n",
    " SPL = np.asarray(SPL)\n",
    " TOL_equiv = SPL + coeff\n",
    " p = 10 ** (TOL_equiv / 20) - S_off\n",
    " U_low = (p + b_low) / s_low\n",
    " U_high = (p + b_high) / s_high\n",
    " return np.where(U_low > 10, U_high, U_low)\n",
    "def band_integration_coeff(fc):\n",
    " freq_min = fc * 2 ** (-1/6)\n",
    " freq_max = fc * 2 ** (1/6)\n",
    " return 10 * np.log10(freq_max - freq_min)\n",
    "# --- === prepare data for each fit === ---\n",
    "# --- Nystuen, pensieri, vagle use 8000 hz ---\n",
    "filt_8000 = (\n",
    " (float_data[\"distance_km\"] < 40) &\n",
    " (~float_data[\"f_8000Hz_corrected\"].isna()) &\n",
    " (~float_data[\"U_DYFAMED\"].isna())\n",
    ")\n",
    "x_8000 = float_data.loc[filt_8000, \"f_8000Hz_corrected\"].values\n",
    "y_8000 = float_data.loc[filt_8000, \"U_DYFAMED\"].values\n",
    "# --- Cauchy uses 3150 hz ---\n",
    "filt_3150 = (\n",
    " (float_data[\"distance_km\"] < 40) &\n",
    " (~float_data[\"f_3150Hz_corrected\"].isna()) &\n",
    " (~float_data[\"U_DYFAMED\"].isna())\n",
    ")\n",
    "x_3150 = float_data.loc[filt_3150, \"f_3150Hz_corrected\"].values\n",
    "y_3150 = float_data.loc[filt_3150, \"U_DYFAMED\"].values\n",
    "# --- Pensieri model is only defined for spl in [30, 60) ---\n",
    "pensieri_mask = (x_8000 >= 30) & (x_8000 < 60)\n",
    "x_pensieri_fit = x_8000[pensieri_mask]\n",
    "y_pensieri_fit = y_8000[pensieri_mask]\n",
    "# --- === initial parameter guesses === ---\n",
    "nystuen_guess = [0.0005, -0.0310, 0.4904, 2.0871]\n",
    "pensieri_guess = [0.1458, -3.146, 0.044642, -3.2917, 63.016]\n",
    "vagle_guess = [7.38, -38.70, 21.69]\n",
    "fc = 3150\n",
    "coeff_cauchy = band_integration_coeff(fc)\n",
    "S_off_init = np.nanmin(10 ** (float_data[\"f_3150Hz\"].dropna()/20))\n",
    "cauchy_guess = [S_off_init, coeff_cauchy, 0.4e4, 0.2e4, 1.6e4, 12.5e4]\n",
    "# --- === curve fitting === ---\n",
    "# --- Nystuen (fit to all filtered 8000hz data) ---\n",
    "popt_nystuen, _ = curve_fit(nystuen2015_fit, x_8000, y_8000, p0 = nystuen_guess)\n",
    "# --- Pensieri (fit only in [30, 60) spl) ---\n",
    "popt_pensieri, _ = curve_fit(pensieri2015_fit, x_pensieri_fit, y_pensieri_fit, p0 = pensieri_guess)\n",
    "# --- Vagle (fit to all filtered 8000hz data) ---\n",
    "popt_vagle, _ = curve_fit(vagle1990_fit, x_8000, y_8000, p0 = vagle_guess)\n",
    "# --- Cauchy (fit to all filtered 3150hz data, but only free the last four params) ---\n",
    "def cauchy2018_fit_partial(SPL, s_low, b_low, s_high, b_high):\n",
    " return cauchy2018_fit(SPL, S_off_init, coeff_cauchy, s_low, b_low, s_high, b_high)\n",
    "popt_cauchy, _ = curve_fit(cauchy2018_fit_partial, x_3150, y_3150, p0=[0.4e4, 0.2e4, 1.6e4, 12.5e4])\n",
    "# --- === apply all models to the full dataset === ---\n",
    "float_data[\"U_nystuen_opt_coeffs\"] = nystuen2015_fit(float_data[\"f_8000Hz_corrected\"].values, *popt_nystuen)\n",
    "float_data[\"U_pensieri_opt_coeffs\"] = pensieri2015_fit(float_data[\"f_8000Hz_corrected\"].values, *popt_pensieri)\n",
    "float_data[\"U_vagle_opt_coeffs\"] = vagle1990_fit(float_data[\"f_8000Hz_corrected\"].values, *popt_vagle)\n",
    "float_data[\"U_cauchy_opt_coeffs\"] = cauchy2018_fit(\n",
    " float_data[\"f_3150Hz_corrected\"].values,\n",
    " S_off_init, coeff_cauchy, *popt_cauchy\n",
    ")\n",
    "# --- === print fitted coefficients === ---\n",
    "print(\"Nystuen coefficients:\", popt_nystuen)\n",
    "print(\"Pensieri coefficients:\", popt_pensieri)\n",
    "print(\"Vagle coefficients:\", popt_vagle)\n",
    "print(\"Cauchy coefficients (s_low, b_low, s_high, b_high):\", popt_cauchy)\n",
    "print(\"Cauchy S_off (fixed):\", S_off_init, \"coeff (fixed):\", coeff_cauchy)\n",
    "# --- === plot for quick validation === ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_nystuen_opt_coeffs\"], alpha = 0.3, label=\"Nystuen\")\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_pensieri_opt_coeffs\"], alpha = 0.3, label=\"Pensieri\")\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_vagle_opt_coeffs\"], alpha = 0.3, label=\"Vagle\")\n",
    "plt.scatter(float_data[\"U_DYFAMED\"], float_data[\"U_cauchy_opt_coeffs\"], alpha = 0.3, label=\"Cauchy\")\n",
    "plt.plot(\n",
    " [float_data[\"U_DYFAMED\"].min(), float_data[\"U_DYFAMED\"].max()],\n",
    " [float_data[\"U_DYFAMED\"].min(), float_data[\"U_DYFAMED\"].max()],\n",
    " 'k--', label=\"1:1 line\"\n",
    ")\n",
    "plt.xlabel(\"Reference Wind Speed (U_DYFAMED)\")\n",
    "plt.ylabel(\"Model Wind Speed (optimized)\")\n",
    "plt.legend()\n",
    "plt.title(\"Optimized Wind Speed Model Fits\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc7e14",
   "metadata": {},
   "source": [
    "## 9 - Process AIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f07dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: parse datetime ---\n",
    "float_data['datetime_DO'] = pd.to_datetime(float_data['datetime_DO'], errors='coerce')\n",
    "float_data['nearby_ships'] = np.nan # prefill with NaN\n",
    "# --- Step 2: interpolate trajectory ---\n",
    "df = float_data.copy()\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "stations_df = (\n",
    " df[[\"station_number\", \"latitude\", \"longitude\", \"datetime_DO\"]]\n",
    " .drop_duplicates(subset=[\"station_number\", \"latitude\", \"longitude\"])\n",
    " .sort_values(\"datetime_DO\")\n",
    " .reset_index(drop = True)\n",
    ")\n",
    "stations_df[\"datetime_DO\"] = pd.to_datetime(stations_df[\"datetime_DO\"])\n",
    "interpolated_rows = []\n",
    "for i in range(len(stations_df) - 1):\n",
    " start = stations_df.iloc[i]\n",
    " end = stations_df.iloc[i + 1]\n",
    " start_time = start[\"datetime_DO\"]\n",
    " end_time = end[\"datetime_DO\"]\n",
    " duration_hours = int((end_time - start_time).total_seconds() // 3600)\n",
    " if duration_hours <= 0:\n",
    " continue\n",
    " lonlats = geod.npts(\n",
    " start[\"longitude\"], start[\"latitude\"],\n",
    " end[\"longitude\"], end[\"latitude\"],\n",
    " duration_hours - 1\n",
    " )\n",
    " interpolated_rows.append({\n",
    " \"datetime\": start_time,\n",
    " \"latitude_interp\": start[\"latitude\"],\n",
    " \"longitude_interp\": start[\"longitude\"]\n",
    " })\n",
    " for j, (lon, lat) in enumerate(lonlats):\n",
    " interpolated_rows.append({\n",
    " \"datetime\": start_time + timedelta(hours = j + 1),\n",
    " \"latitude_interp\": lat,\n",
    " \"longitude_interp\": lon\n",
    " })\n",
    "interpolated_rows.append({\n",
    " \"datetime\": stations_df.iloc[-1][\"datetime_DO\"],\n",
    " \"latitude_interp\": stations_df.iloc[-1][\"latitude\"],\n",
    " \"longitude_interp\": stations_df.iloc[-1][\"longitude\"]\n",
    "})\n",
    "interp_df = pd.DataFrame(interpolated_rows)\n",
    "interp_df[\"datetime\"] = pd.to_datetime(interp_df[\"datetime\"])\n",
    "float_data = float_data.sort_values(\"datetime_DO\")\n",
    "interp_df = interp_df.sort_values(\"datetime\")\n",
    "# --- Merge interpolated trajectory with float_data ---\n",
    "float_data = pd.merge_asof(\n",
    " float_data,\n",
    " interp_df,\n",
    " left_on=\"datetime_DO\",\n",
    " right_on=\"datetime\",\n",
    " direction=\"nearest\"\n",
    ")\n",
    "# --- Step 3: process ais data ---\n",
    "radius_km = 20\n",
    "time_window_minutes = 30\n",
    "data_folder = \"data/AIS/parsed/\"\n",
    "bounding_box = (6.5, 8.0, 43.0, 44.0)\n",
    "for filepath in glob.glob(os.path.join(data_folder, \"ais_positions_*.csv\")):\n",
    " print(f\"Processing {filepath}\")\n",
    " try:\n",
    " ais = pd.read_csv(filepath, on_bad_lines='skip', low_memory = False)\n",
    " ais = ais.dropna(subset=['utc_hour', 'utc_min', 'x', 'y'])\n",
    " west, east, south, north = bounding_box\n",
    " ais = ais[\n",
    " (ais['x'] >= west) & (ais['x'] <= east) &\n",
    " (ais['y'] >= south) & (ais['y'] <= north)\n",
    " ]\n",
    " ais['datetime'] = pd.to_datetime(dict(\n",
    " year = ais['file_year'].astype(int),\n",
    " month = ais['file_month'].astype(int),\n",
    " day = ais['file_day'].astype(int),\n",
    " hour = ais['utc_hour'].astype(int),\n",
    " minute = ais['utc_min'].astype(int)\n",
    " ), errors='coerce')\n",
    " if ais.empty:\n",
    " continue\n",
    " ais_date = ais['datetime'].dt.date.unique()[0]\n",
    " float_mask = float_data['datetime_DO'].dt.date == ais_date\n",
    " if not float_mask.any():\n",
    " continue\n",
    " float_subset = float_data[float_mask].copy()\n",
    " ship_counts = []\n",
    " for _, float_row in float_subset.iterrows():\n",
    " lat_f = float_row['latitude_interp']\n",
    " lon_f = float_row['longitude_interp']\n",
    " time_f = float_row['datetime_DO']\n",
    " if pd.isna(time_f) or pd.isna(lat_f) or pd.isna(lon_f):\n",
    " ship_counts.append(np.nan)\n",
    " continue\n",
    " start = time_f - pd.Timedelta(minutes = time_window_minutes)\n",
    " end = time_f + pd.Timedelta(minutes = time_window_minutes)\n",
    " ais_window = ais[(ais['datetime'] >= start) & (ais['datetime'] <= end)]\n",
    " ais_dedup = ais_window.sort_values('datetime').drop_duplicates(subset='mmsi', keep='first')\n",
    " distances = ais_dedup.apply(\n",
    " lambda row: haversine((lat_f, lon_f), (row['y'], row['x']), unit = Unit.KILOMETERS),\n",
    " axis = 1\n",
    " )\n",
    " ship_counts.append((distances <= radius_km).sum())\n",
    " float_data.loc[float_mask, 'nearby_ships'] = ship_counts\n",
    " except Exception as e:\n",
    " print(f\"Error in {filepath}: {e}\")\n",
    " continue\n",
    "# --- Step 4: highlighting and plotting deviations ---\n",
    "float_data['U_diff'] = float_data['U_nystuen_opt_coeffs'] - float_data['U_DYFAMED']\n",
    "float_data['datetime'] = pd.to_datetime(float_data['datetime'])\n",
    "float_data = float_data.sort_values('datetime')\n",
    "rmse = (float_data['U_diff']**2).mean()**0.5\n",
    "ship_times = float_data[float_data['nearby_ships'] > 0]['datetime']\n",
    "def highlight_condition(dt, diff):\n",
    " is_significant = abs(diff) > rmse\n",
    " is_ship_nearby = any(abs((dt - ship_time).total_seconds()) <= 1800 for ship_time in ship_times)\n",
    " return is_significant and is_ship_nearby\n",
    "float_data['highlight'] = float_data.apply(\n",
    " lambda row: highlight_condition(row['datetime'], row['U_diff']), axis = 1\n",
    ")\n",
    "# --- Step 5: plot ---\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "current_color = None\n",
    "segment_x, segment_y = [], []\n",
    "for idx, row in float_data.iterrows():\n",
    " color = 'red' if row['highlight'] else 'tab:blue'\n",
    " if current_color is None:\n",
    " current_color = color\n",
    " if color != current_color and segment_x:\n",
    " ax.plot(segment_x, segment_y, color = current_color)\n",
    " segment_x, segment_y = [], []\n",
    " current_color = color\n",
    " segment_x.append(row['datetime'])\n",
    " segment_y.append(row['U_diff'])\n",
    "if segment_x:\n",
    " ax.plot(segment_x, segment_y, color = current_color)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('U Difference (m/s)')\n",
    "ax.set_title(f'Deviation U Float - U DYFAMED (Red = |Diff| > RMSE ≈ {rmse:.2f} & Ship Nearby)')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --- Step 6: remove suspect data ---\n",
    "float_data = float_data[~float_data['highlight']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48baef",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sample some viridis colors ---\n",
    "color1 = cm.viridis(0.1)\n",
    "color2 = cm.viridis(0.5)\n",
    "color3 = cm.viridis(0.7)\n",
    "color4 = cm.viridis(0.9)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 10), dpi = 300, sharex = True)\n",
    "# --- === top subplot: unoptimized models === ---\n",
    "axs[0].plot(wind_data[\"datenum\"], wind_data[\"U_DYFAMED\"], c='k', linestyle='-', label=\"DYFAMED buoy\")\n",
    "axs[0].plot(float_data[\"datenum\"], float_data[\"U_vagle_litt_coeffs\"], linestyle='-', c = color1, label=\"Vagle et al. (1990)\")\n",
    "axs[0].plot(float_data[\"datenum\"], float_data[\"U_nystuen_litt_coeffs\"], c = color2, label=\"Nystuen et al. (2015)\")\n",
    "axs[0].plot(float_data[\"datenum\"], float_data[\"U_pensieri_litt_coeffs\"], c = color3, label=\"Pensieri et al. (2015)\")\n",
    "axs[0].plot(float_data[\"datenum\"], float_data[\"U_cauchy_litt_coefs\"], c = color4, label=\"Cauchy et al. (2018)\")\n",
    "axs[0].axvline(deployment_boundary, color='black', linestyle='--', linewidth = 1, label=\"Start of deployment B\")\n",
    "axs[0].set_ylabel(r\"Wind speed$_{\\mathrm{}}$ (m s$^{-1}$)\")\n",
    "axs[0].grid(alpha = 0.3)\n",
    "axs[0].set_xlim(float_data[\"datenum\"].min(), float_data[\"datenum\"].max())\n",
    "axs[0].set_ylim(0, 30)\n",
    "axs[0].text(-0.05, 1.05, \"a)\", transform = axs[0].transAxes, fontsize = 14, va=\"bottom\")\n",
    "# --- === bottom subplot: optimized models === ---\n",
    "axs[1].plot(wind_data[\"datenum\"], wind_data[\"U_DYFAMED\"], c='k', linestyle='-', label=\"DYFAMED buoy\")\n",
    "axs[1].plot(float_data[\"datenum\"], float_data[\"U_vagle_opt_coeffs\"], c = color1, linestyle='-', label=\"Vagle et al. (1990)\")\n",
    "axs[1].plot(float_data[\"datenum\"], float_data[\"U_nystuen_opt_coeffs\"], c = color2, label=\"Nystuen et al. (2015)\")\n",
    "axs[1].plot(float_data[\"datenum\"], float_data[\"U_pensieri_opt_coeffs\"], c = color3, label=\"Pensieri et al. (2015)\")\n",
    "axs[1].plot(float_data[\"datenum\"], float_data[\"U_cauchy_opt_coeffs\"], c = color4, label=\"Cauchy et al. (2018)\")\n",
    "axs[1].axvline(deployment_boundary, color='black', linestyle='--', linewidth = 1, label=\"Start of deployment B\")\n",
    "# --- Axs[1].set_xlabel(\"datetime\") ---\n",
    "axs[1].set_ylabel(r\"Wind speed$_{\\mathrm{}}$ (m s$^{-1}$)\")\n",
    "axs[1].grid(alpha = 0.3)\n",
    "axs[1].set_xlim(float_data[\"datenum\"].min(), float_data[\"datenum\"].max())\n",
    "axs[1].set_ylim(0, 30)\n",
    "axs[1].text(-0.05, 1.05, \"b)\", transform = axs[1].transAxes, fontsize = 14, va=\"bottom\")\n",
    "# --- Format shared x-axis ---\n",
    "axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "axs[1].xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "fig.autofmt_xdate()\n",
    "# --- === shared legend below subplots === ---\n",
    "handles1, labels1 = axs[0].get_legend_handles_labels()\n",
    "handles2, labels2 = axs[1].get_legend_handles_labels()\n",
    "# --- Use dict to remove duplicates ---\n",
    "combined = dict(zip(labels1 + labels2, handles1 + handles2))\n",
    "fig.legend(combined.values(), combined.keys(), loc=\"lower center\", bbox_to_anchor=(0.5, 0.01), ncol = 6, fontsize = 11)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom = 0.15) # Leave space for the legend\n",
    "fig.savefig(\"figs/unpot_vs_opt.png\", dpi = 300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edced0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a 2x2 grid of subplots ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), dpi = 300)\n",
    "# --- Common axis limits ---\n",
    "xlim = (0, 30)\n",
    "ylim = (0, 30)\n",
    "# --- Colormap for distance coloring ---\n",
    "cmap = \"viridis\"\n",
    "# --- Function to add regression stats to a plot ---\n",
    "def add_regression_stats(ax, x, y):\n",
    " mask = (~x.isna()) & (~y.isna())\n",
    " x_clean, y_clean = x[mask], y[mask]\n",
    " if len(x_clean) > 1:\n",
    " slope, intercept, r_value, _, _ = stats.linregress(x_clean, y_clean)\n",
    " r_squared = r_value**2\n",
    " text_str = f\"Slope: {slope:.2f}\\nIntercept: {intercept:.2f}\\nR²: {r_squared:.2f}\"\n",
    " ax.text(0.05, 0.85, text_str, transform = ax.transAxes, fontsize = 10,\n",
    " bbox = dict(facecolor='white', alpha = 0.6))\n",
    "# --- Models and titles in desired order (row-wise) ---\n",
    "models = [\n",
    " (\"U_vagle_opt_coeffs\", \"Vagle et al. (1990)\"),\n",
    " (\"U_nystuen_opt_coeffs\", \"Nystuen et al. (2015)\"),\n",
    " (\"U_pensieri_opt_coeffs\", \"Pensieri et al. (2015)\"),\n",
    " (\"U_cauchy_opt_coeffs\", \"Cauchy et al. (2018)\")\n",
    "]\n",
    "# --- Buoy wind speed data (x-axis) ---\n",
    "buoy_wind_speed = float_data[\"U_DYFAMED\"]\n",
    "# --- Plot each model in the 2x2 grid ---\n",
    "sc = None\n",
    "for i, (ax, (model, title)) in enumerate(zip(axes.flatten(), models)):\n",
    " model_wind_speed = float_data[model]\n",
    " sc = ax.scatter(buoy_wind_speed, model_wind_speed, c = float_data[\"distance_km\"],\n",
    " cmap = cmap, vmin = 0, vmax = 80, s = 15, alpha = 0.7)\n",
    "# --- Add regression stats, 1:1 line ---\n",
    " add_regression_stats(ax, buoy_wind_speed, model_wind_speed)\n",
    " ax.plot([xlim[0], xlim[1]], [ylim[0], ylim[1]], linestyle=\"--\", color=\"black\", label=\"1:1 Line\")\n",
    "# --- Axis limits and grid ---\n",
    " ax.set_xlim(xlim)\n",
    " ax.set_ylim(ylim)\n",
    " ax.grid(alpha = 0.3)\n",
    "# --- Ax.set_title(title, fontsize = 11) ---\n",
    "# --- Only set labels on outer edges ---\n",
    " if i in [0, 2]:\n",
    " ax.set_ylabel(r\"Float wind speed (m s$^{-1}$)\")\n",
    " if i in [2, 3]:\n",
    " ax.set_xlabel(r\"Buoy wind speed (m s$^{-1}$)\")\n",
    "# --- Add subplot label with model name ---\n",
    " label = f\"{string.ascii_lowercase[i]}) {title}\"\n",
    " ax.text(-0.1, 1.05, label, transform = ax.transAxes,\n",
    " fontsize = 11, va=\"bottom\", ha=\"left\")\n",
    "# --- Adjust layout ---\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "# --- Add horizontal colorbar below the subplots ---\n",
    "cbar_ax = fig.add_axes([0.25, 0.02, 0.5, 0.015])\n",
    "cbar = fig.colorbar(sc, cax = cbar_ax, orientation='horizontal')\n",
    "cbar.set_label(\"Distance from DYFAMED (km)\")\n",
    "cbar.set_ticks([0, 20, 40, 60, 80])\n",
    "fig.savefig(\"figs/optimized_scatter.png\", dpi = 300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b150eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- === statistics === ---\n",
    "# --- Calculate offset (difference between buoy wind speed and each model's prediction) ---\n",
    "float_data[\"offset_Nystuen\"] = matched_wind_data[\"U_DYFAMED\"] - float_data[\"U_nystuen_opt_coeffs\"]\n",
    "float_data[\"offset_Vagle\"] = matched_wind_data[\"U_DYFAMED\"] - float_data[\"U_vagle_opt_coeffs\"]\n",
    "float_data[\"offset_Pensieri\"] = matched_wind_data[\"U_DYFAMED\"] - float_data[\"U_pensieri_opt_coeffs\"]\n",
    "float_data[\"offset_Cauchy\"] = matched_wind_data[\"U_DYFAMED\"] - float_data[\"U_cauchy_opt_coeffs\"]\n",
    "# --- Compute statistical metrics ---\n",
    "stats = {\n",
    " \"Model\": [\n",
    " \"Nystuen 2015\", \"Vagle 1990\", \"Pensieri 2015\", \"Cauchy 2018\"\n",
    " ],\n",
    " \"Mean Offset\": [\n",
    " np.nanmean(float_data[\"offset_Nystuen\"]),\n",
    " np.nanmean(float_data[\"offset_Vagle\"]),\n",
    " np.nanmean(float_data[\"offset_Pensieri\"]),\n",
    " np.nanmean(float_data[\"offset_Cauchy\"])\n",
    " ],\n",
    " \"Median Offset\": [\n",
    " np.nanmedian(float_data[\"offset_Nystuen\"]),\n",
    " np.nanmedian(float_data[\"offset_Vagle\"]),\n",
    " np.nanmedian(float_data[\"offset_Pensieri\"]),\n",
    " np.nanmedian(float_data[\"offset_Cauchy\"]),\n",
    " ],\n",
    " \"RMSE\": [\n",
    " np.sqrt(np.nanmean(float_data[\"offset_Nystuen\"]**2)),\n",
    " np.sqrt(np.nanmean(float_data[\"offset_Vagle\"]**2)),\n",
    " np.sqrt(np.nanmean(float_data[\"offset_Pensieri\"]**2)),\n",
    " np.sqrt(np.nanmean(float_data[\"offset_Cauchy\"]**2)),\n",
    " ]\n",
    "}\n",
    "# --- Convert to dataframe for better visualization ---\n",
    "stats_df = pd.DataFrame(stats)\n",
    "# --- Determine the best-performing model (lowest rmse) ---\n",
    "best_model = stats_df.loc[stats_df[\"RMSE\"].idxmin(), \"Model\"]\n",
    "print(\"Best model = {}\".format(best_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d1650",
   "metadata": {},
   "source": [
    "## 10 - Match ERA5 to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ===== import era5 data ===== ---\n",
    "era = xr.load_dataset(\"data/era5.nc\")\n",
    "era[\"wind_speed\"] = np.sqrt(era[\"u10\"]**2 + era[\"v10\"]**2)\n",
    "era[\"datetime\"] = pd.to_datetime(era.valid_time.values)\n",
    "# --- Extract min/max valid times as pandas timestamps ---\n",
    "era_min_time = pd.Timestamp(era[\"datetime\"].min().values)\n",
    "era_max_time = pd.Timestamp(era[\"datetime\"].max().values)\n",
    "def match_era_wind(df):\n",
    " \"\"\"Find nearest datetime and lat/lon match for wind speed from ERA, ensuring valid time window.\"\"\"\n",
    " df = df.copy()\n",
    "# --- Dynamically detect the datetime column ---\n",
    " if \"datetime_DO\" in df.columns:\n",
    " df[\"datetime\"] = pd.to_datetime(df[\"datetime_DO\"], errors='coerce')\n",
    " elif \"datetime\" in df.columns:\n",
    " df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors='coerce')\n",
    " else:\n",
    " raise ValueError(\"No datetime column found in the dataset.\")\n",
    "# --- Detect latitude and longitude column names ---\n",
    " lat_col = \"Latitude\" if \"Latitude\" in df.columns else \"lat\" if \"lat\" in df.columns else \"latitude\"\n",
    " lon_col = \"Longitude\" if \"Longitude\" in df.columns else \"lon\" if \"lon\" in df.columns else \"longitude\"\n",
    " if lat_col not in df.columns or lon_col not in df.columns:\n",
    " raise ValueError(\"Latitude/Longitude columns are missing in the dataset.\")\n",
    "# --- Filter rows within era5 time bounds ---\n",
    " valid_rows = (df[\"datetime\"] >= era_min_time) & (df[\"datetime\"] <= era_max_time)\n",
    "# --- Initialize output column ---\n",
    " df[\"wind_speed_era\"] = np.nan\n",
    "# --- Loop through valid rows with progress bar ---\n",
    " for i, row in tqdm(df.loc[valid_rows].iterrows(), total = valid_rows.sum(), desc=\"Matching ERA5 Wind Speed\"):\n",
    " lat, lon = row[lat_col], row[lon_col]\n",
    " nearest_wind = era.sel(valid_time = row[\"datetime\"], latitude = lat, longitude = lon, method=\"nearest\")[\"wind_speed\"].values\n",
    " if isinstance(nearest_wind, np.ndarray) and nearest_wind.size > 0:\n",
    " df.at[i, \"wind_speed_era\"] = nearest_wind.item()\n",
    " else:\n",
    " df.at[i, \"wind_speed_era\"] = np.nan\n",
    " return df[\"wind_speed_era\"]\n",
    "# --- Apply function to float_data ---\n",
    "float_data[\"U_ERA5\"] = match_era_wind(float_data)\n",
    "# --- Apply function to wind_data ---\n",
    "wind_data[\"U_ERA5\"] = match_era_wind(wind_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570601e",
   "metadata": {},
   "source": [
    "### 11 - Train machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- === step 1: clean & filter data === ---\n",
    "valid = float_data[['f_8000Hz_corrected', 'U_ERA5', 'U_DYFAMED', 'datenum', 'distance_km', 'datetime_DO']].dropna()\n",
    "numeric_cols = ['f_8000Hz_corrected', 'U_ERA5', 'U_DYFAMED', 'datenum', 'distance_km']\n",
    "valid = valid[valid[numeric_cols].applymap(np.isfinite).all(axis = 1)]\n",
    "# --- === step 2: extract variables === ---\n",
    "SPL8 = valid['f_8000Hz_corrected'].values\n",
    "U_ERA5 = valid['U_ERA5'].values\n",
    "U_DYFAMED = valid['U_DYFAMED'].values\n",
    "datenum = valid['datenum'].values\n",
    "datetime = pd.to_datetime(valid['datetime_DO'])\n",
    "distance = valid['distance_km'].values\n",
    "# --- === constants === ---\n",
    "n_bootstrap = 100\n",
    "era5_std = 1.5 # ERA5 input uncertainty\n",
    "# --- === define nystuen model === ---\n",
    "def nystuen_model(SPL8, a3, a2, a1, a0):\n",
    " return a3*SPL8**3 + a2*SPL8**2 + a1*SPL8 + a0\n",
    "# --- === step 3: bootstrap nystuen model with total uncertainty === ---\n",
    "print(\"Bootstrapping Nystuen model with full input noise and re-fitting...\")\n",
    "U_nystuen_bootstrap = np.zeros((n_bootstrap, len(SPL8)))\n",
    "for i in tqdm(range(n_bootstrap)):\n",
    "# --- Resample indices and add noise to u_era5 ---\n",
    " idx = np.random.choice(len(SPL8), size = len(SPL8), replace = True)\n",
    " SPL8_sample = SPL8[idx]\n",
    " U_ERA5_sample = U_ERA5[idx] + np.random.normal(0, era5_std, size = len(idx))\n",
    " try:\n",
    "# --- Fit model ---\n",
    " popt, _ = curve_fit(nystuen_model, SPL8_sample, U_ERA5_sample)\n",
    "# --- Predict on clean spl8 with some additional prediction noise (optional) ---\n",
    " SPL8_pred = SPL8 + np.random.normal(0, 0.0, size = len(SPL8)) # can tune if SPL8 has uncertainty\n",
    " U_nystuen_bootstrap[i] = nystuen_model(SPL8_pred, *popt)\n",
    " except RuntimeError:\n",
    " U_nystuen_bootstrap[i] = np.nan\n",
    "# --- === step 4: compute mean and std of bootstrapped nystuen predictions === ---\n",
    "U_nystuen_mean = np.nanmean(U_nystuen_bootstrap, axis = 0)\n",
    "U_nystuen_std = np.nanstd(U_nystuen_bootstrap, axis = 0)\n",
    "# --- === step 5: residuals = dyfamed - nystuen mean === ---\n",
    "residuals = U_DYFAMED - U_nystuen_mean\n",
    "# --- === step 6: prepare ml input features === ---\n",
    "datenum_norm = (datenum - datenum.min()) / (datenum.max() - datenum.min())\n",
    "X_base = np.column_stack((SPL8, U_ERA5, datenum_norm, U_nystuen_mean))\n",
    "y = residuals\n",
    "# --- === step 7: define training set === ---\n",
    "train_mask = (distance <= 40)\n",
    "X_train_base = X_base[train_mask]\n",
    "y_train = y[train_mask]\n",
    "# --- === step 8: bootstrap ml models === ---\n",
    "residual_preds = np.zeros((n_bootstrap, X_base.shape[0]))\n",
    "print(\"Bootstrapping XGB models...\")\n",
    "for i in tqdm(range(n_bootstrap)):\n",
    " idx = np.random.choice(len(X_train_base), size = len(X_train_base), replace = True)\n",
    " X_boot = X_train_base[idx].copy()\n",
    " X_boot[:, 1] += np.random.normal(0, era5_std, size = X_boot.shape[0]) # Add noise to U_ERA5 feature\n",
    " model = XGBRegressor(\n",
    " n_estimators = 300,\n",
    " learning_rate = 0.05,\n",
    " max_depth = 3,\n",
    " subsample = 0.9,\n",
    " colsample_bytree = 0.8,\n",
    " random_state = i\n",
    " )\n",
    " model.fit(X_boot, y_train[idx])\n",
    "# --- Predict with noisy u_era5 input ---\n",
    " X_pred = X_base.copy()\n",
    " X_pred[:, 1] += np.random.normal(0, era5_std, size = X_pred.shape[0])\n",
    " residual_preds[i] = model.predict(X_pred)\n",
    "# --- === step 9: combine ml + era5 uncertainty === ---\n",
    "residual_mean = residual_preds.mean(axis = 0)\n",
    "residual_std = residual_preds.std(axis = 0)\n",
    "U_corrected_boot_mean = U_nystuen_mean + residual_mean\n",
    "U_combined_std = np.sqrt(residual_std**2 + era5_std**2)\n",
    "# --- === step 10: store in float_data === ---\n",
    "float_data.loc[valid.index, \"U_nystuen_opt_coeffs\"] = U_nystuen_mean\n",
    "float_data.loc[valid.index, \"U_nystuen_opt_std\"] = U_nystuen_std\n",
    "float_data.loc[valid.index, \"U_corrected_XGB_boot_mean\"] = U_corrected_boot_mean\n",
    "float_data.loc[valid.index, \"U_corrected_XGB_boot_std\"] = U_combined_std\n",
    "# --- === step 11: plot comparison === ---\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = float_data.loc[valid.index, \"datenum\"]\n",
    "y = float_data.loc[valid.index, \"U_corrected_XGB_boot_mean\"]\n",
    "err = float_data.loc[valid.index, \"U_corrected_XGB_boot_std\"]\n",
    "y_nyst = float_data.loc[valid.index, \"U_nystuen_opt_coeffs\"]\n",
    "err_nyst = float_data.loc[valid.index, \"U_nystuen_opt_std\"]\n",
    "# --- Dyfamed buoy (truth) ---\n",
    "ax.plot(wind_data[\"datenum\"], wind_data[\"U_DYFAMED\"], label=\"DYFAMED (True Wind)\", color=\"black\", linestyle=\"--\")\n",
    "# --- Ml-corrected with uncertainty ---\n",
    "ax.plot(x, y, label=\"XGB-Corrected (Mean)\", color=\"firebrick\")\n",
    "ax.fill_between(x, y - err, y + err, color=\"firebrick\", alpha = 0.2, label=\"±σ (ML + ERA5)\")\n",
    "# --- Nystuen with bootstrapped uncertainty ---\n",
    "ax.plot(x, y_nyst, label=\"Nystuen (ERA5 Fit)\", color=\"darkorange\", linestyle=\"--\")\n",
    "ax.fill_between(x, y_nyst - err_nyst, y_nyst + err_nyst, color=\"orange\", alpha = 0.2, label=\"±σ (Bootstrapped)\")\n",
    "# --- Aesthetics ---\n",
    "ax.set_ylabel(\"Wind Speed (m/s)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(\"Wind Speed: DYFAMED vs ERA5 vs ML-Corrected\\nWith Bootstrapped Uncertainty\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a144a",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load wind data with era5 matchup ---\n",
    "wind_data = pd.read_csv(\"data/wind_data_full_ERA5.csv\")\n",
    "# --- First two panels ---\n",
    "mask = float_data[['U_DYFAMED', 'U_nystuen_opt_coeffs', 'U_corrected_XGB_boot_mean']].notna().all(axis = 1)\n",
    "u_true = float_data.loc[mask, 'U_DYFAMED']\n",
    "u_nyst = float_data.loc[mask, 'U_nystuen_opt_coeffs']\n",
    "u_xgb = float_data.loc[mask, 'U_corrected_XGB_boot_mean']\n",
    "dist = float_data.loc[mask, 'distance_km']\n",
    "r2_nyst = r2_score(u_true, u_nyst)\n",
    "rmse_nyst = mean_squared_error(u_true, u_nyst)\n",
    "r2_xgb = r2_score(u_true, u_xgb)\n",
    "rmse_xgb = mean_squared_error(u_true, u_xgb)\n",
    "# --- Third panel ---\n",
    "mask_wind = wind_data[['U_DYFAMED', 'U_ERA5']].notna().all(axis = 1)\n",
    "u_dyf_wind = wind_data.loc[mask_wind, 'U_DYFAMED']\n",
    "u_era5 = wind_data.loc[mask_wind, 'U_ERA5']\n",
    "dist_wind = wind_data.loc[mask_wind, 'distance_km'] if 'distance_km' in wind_data.columns else np.full(len(u_era5), np.nan)\n",
    "r2_era5 = r2_score(u_dyf_wind, u_era5)\n",
    "rmse_era5 = mean_squared_error(u_dyf_wind, u_era5)\n",
    "# --- Plotting ---\n",
    "fig, axs = plt.subplots(\n",
    " 1, 3, figsize=(21, 6), sharex = True, sharey = True, dpi = 300,\n",
    " gridspec_kw={'width_ratios': [1, 1, 0.82]} # Adjust the width of subplot 3 here\n",
    ")\n",
    "# --- Panel a: nystuen ---\n",
    "sc1 = axs[0].scatter(u_true, u_nyst, c = dist, cmap='viridis', alpha = 0.8)\n",
    "axs[0].plot([0, 16], [0, 16], 'k--', lw = 1)\n",
    "axs[0].text(0.5, 14.6, f\"R² = {r2_nyst:.2f}\\nRMSE = {rmse_nyst:.2f}\", fontsize = 12, bbox = dict(facecolor='white', alpha = 0.6))\n",
    "axs[0].text(-0.1, 1.05, \"a)\", transform = axs[0].transAxes, fontsize = 14, va=\"bottom\", ha=\"left\")\n",
    "cbar1 = fig.colorbar(sc1, ax = axs[0])\n",
    "cbar1.set_label(\"Distance from DYFAMED (km)\")\n",
    "# --- Panel b: xgb ---\n",
    "sc2 = axs[1].scatter(u_true, u_xgb, c = dist, cmap='viridis', alpha = 0.8)\n",
    "axs[1].plot([0, 16], [0, 16], 'k--', lw = 1)\n",
    "axs[1].text(0.5, 14.6, f\"R² = {r2_xgb:.2f}\\nRMSE = {rmse_xgb:.2f}\", fontsize = 12, bbox = dict(facecolor='white', alpha = 0.6))\n",
    "axs[1].text(-0.1, 1.05, \"b)\", transform = axs[1].transAxes, fontsize = 14, va=\"bottom\", ha=\"left\")\n",
    "cbar2 = fig.colorbar(sc2, ax = axs[1])\n",
    "cbar2.set_label(\"Distance from DYFAMED (km)\")\n",
    "# --- Panel c: era5 vs dyfamed from wind_data (single-location comparison) ---\n",
    "axs[2].scatter(u_dyf_wind, u_era5, color='black', alpha = 0.8)\n",
    "axs[2].plot([0, 16], [0, 16], 'k--', lw = 1)\n",
    "axs[2].text(0.5, 14.6, f\"R² = {r2_era5:.2f}\\nRMSE = {rmse_era5:.2f}\", fontsize = 12, bbox = dict(facecolor='white', alpha = 0.6))\n",
    "axs[2].text(-0.1, 1.05, \"c)\", transform = axs[2].transAxes, fontsize = 14, va=\"bottom\", ha=\"left\")\n",
    "# --- Axis labels ---\n",
    "axs[0].set_xlabel(\"DYFAMED buoy ($\\mathrm{m\\ s^{-1}}$)\")\n",
    "axs[0].set_ylabel(\"Nystuen et al. (2015 - ERA5) ($\\mathrm{m\\ s^{-1}}$)\")\n",
    "axs[1].set_xlabel(\"DYFAMED buoy ($\\mathrm{m\\ s^{-1}}$)\")\n",
    "axs[1].set_ylabel(\"Nystuen et al. (2015 - ERA5 + XGB) ($\\mathrm{m\\ s^{-1}}$)\")\n",
    "axs[2].set_xlabel(\"DYFAMED buoy ($\\mathrm{m\\ s^{-1}}$)\")\n",
    "axs[2].set_ylabel(\"ERA5 (at DYFAMED) ($\\mathrm{m\\ s^{-1}}$)\")\n",
    "# --- Formatting ---\n",
    "for ax in axs:\n",
    " ax.grid(alpha = 0.3)\n",
    " ax.set_xlim(0, 16)\n",
    " ax.set_ylim(0, 16)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"figs/wind_comparison_scatter_with_ERA5.png\", dpi = 300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c561907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colors ---\n",
    "color_ml = cm.viridis(0.5)\n",
    "color_boot = cm.viridis(0.5)\n",
    "color_nyst = \"xkcd:rose\"\n",
    "color_nyst_shade = \"mistyrose\"\n",
    "color_truth = \"k\"\n",
    "# --- Sections: 20130 to 20185 (in 18-day windows) ---\n",
    "section_bounds = np.arange(20130, 20185, 18)\n",
    "n_sections = len(section_bounds) - 1\n",
    "fig, axs = plt.subplots(n_sections, 1, figsize=(11, 4 * n_sections), dpi = 300)\n",
    "if n_sections == 1:\n",
    " axs = [axs]\n",
    "for i, (start, end) in enumerate(zip(section_bounds[:-1], section_bounds[1:])):\n",
    " ax = axs[i]\n",
    "# --- Plot range ---\n",
    " ax.set_xlim(start, end)\n",
    " ax.set_ylim(0, 18)\n",
    "# --- Slice data in range ---\n",
    " mask = (float_data[\"datenum\"] >= start) & (float_data[\"datenum\"] <= end)\n",
    " datenums = float_data.loc[mask, \"datenum\"]\n",
    " distances = float_data.loc[mask, \"distance_km\"]\n",
    "# --- === plot data === ---\n",
    " ax.plot(wind_data[\"datenum\"], wind_data[\"U_DYFAMED\"], label=\"DYFAMED buoy\", color = color_truth, linestyle=\"-\")\n",
    "# --- Nystuen ±1σ ---\n",
    " y_nyst = float_data[\"U_nystuen_opt_coeffs\"]\n",
    " err_nyst = float_data[\"U_nystuen_opt_std\"]\n",
    " ax.plot(float_data[\"datenum\"], y_nyst, label=\"Nystuen et al. (2015 - ERA5)\", color = color_nyst, linestyle=\"--\")\n",
    " ax.fill_between(float_data[\"datenum\"], y_nyst - err_nyst, y_nyst + err_nyst,\n",
    " color = color_nyst_shade, alpha = 0.3, label=\"_nolegend_\")\n",
    "# --- Ml-corrected ±1σ ---\n",
    " y_ml = float_data[\"U_corrected_XGB_boot_mean\"]\n",
    " err_ml = float_data[\"U_corrected_XGB_boot_std\"]\n",
    " ax.plot(float_data[\"datenum\"], y_ml, label=\"ERA5 + ML (XGB)\", color = color_ml)\n",
    " ax.fill_between(float_data[\"datenum\"], y_ml - err_ml, y_ml + err_ml,\n",
    " color = color_boot, alpha = 0.2, label=\"_nolegend_\")\n",
    "# --- Axis settings ---\n",
    " ax.set_ylabel(r\"Wind speed (m s$^{-1}$)\")\n",
    " ax.grid(alpha = 0.3)\n",
    " ax.set_ylim(0, 18)\n",
    "# --- X-axis formatting ---\n",
    " ax.xaxis_date()\n",
    " ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    " ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    " plt.setp(ax.get_xticklabels(), visible = True, rotation = 30, ha='right')\n",
    "# --- Subplot label (a), (b), ... ---\n",
    " label = f\"{string.ascii_lowercase[i]})\"\n",
    " ax.text(-0.1, 1.15, label, transform = ax.transAxes, fontsize = 12, va=\"bottom\")\n",
    "# --- === add distance axis (top x-axis) === ---\n",
    " twin_ax = ax.twiny()\n",
    " twin_ax.set_xlim(start, end)\n",
    " if not distances.empty:\n",
    " twin_ax.set_xticks(np.linspace(start, end, 4))\n",
    " interp_dist = np.interp(np.linspace(start, end, 4), datenums, distances)\n",
    " twin_ax.set_xticklabels([f\"{d:.0f} km\" for d in interp_dist])\n",
    " else:\n",
    " twin_ax.set_xticks([])\n",
    " twin_ax.set_xticklabels([])\n",
    " if i == 0:\n",
    " twin_ax.set_xlabel(\"Distance from DYFAMED (km)\")\n",
    " twin_ax.xaxis.set_label_position(\"top\")\n",
    " twin_ax.tick_params(axis='x', which='both', labelsize = 10)\n",
    "# --- Optional: add vertical deployment line ---\n",
    " if i == 1:\n",
    " dep_handle = ax.axvline(deployment_boundary, color='black', linestyle='--', linewidth = 1,\n",
    " label=\"Start of deployment B\")\n",
    "# --- Final x-limit and shared legend ---\n",
    "axs[-1].set_xlim(start, 20180)\n",
    "# --- Gather handles for legend ---\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "if 'dep_handle' in locals():\n",
    " handles.append(dep_handle)\n",
    " labels.append(\"Start of deployment B\")\n",
    "fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.54, -0.03), ncol = 4)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom = 0.07, hspace = 0.7)\n",
    "fig.savefig(\"figs/wind_comparison_panel.png\", dpi = 300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4497a",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee449f",
   "metadata": {},
   "source": [
    "#### Extract first profile down to 1000m (station 45) and interpolate at 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter and prepare data for first 1000m station ---\n",
    "# --- Extract data for station 045 ---\n",
    "station_045_profile = float_data[float_data['station_number'] == '045']\n",
    "# --- Keep only rows with non-null temperature and salinity ---\n",
    "filtered_profile = station_045_profile[\n",
    " station_045_profile['temperature'].notna() &\n",
    " station_045_profile['salinity'].notna()\n",
    "]\n",
    "# --- Keep only ascending profiles (e.g., upcast) ---\n",
    "filtered_profile = filtered_profile[filtered_profile['phasename'] == 'ASC']\n",
    "# --- Sort by pressure (ascending) and select relevant columns ---\n",
    "filtered_profile = filtered_profile.sort_values(by='pressure')\n",
    "filtered_profile = filtered_profile[['pressure', 'temperature', 'salinity']]\n",
    "# --- Reset index after filtering and sorting ---\n",
    "filtered_profile = filtered_profile.reset_index(drop = True)\n",
    "# --- Prepare for interpolation ---\n",
    "# --- Rename for clarity ---\n",
    "cleaned_profile = filtered_profile\n",
    "# --- Extract values for interpolation ---\n",
    "pressures = cleaned_profile['pressure'].values\n",
    "temperatures = cleaned_profile['temperature'].values\n",
    "salinities = cleaned_profile['salinity'].values\n",
    "# --- Define interpolation depths: 0 to 1000 m at 1 m resolution ---\n",
    "interp_depths = np.arange(0, 1001, 1)\n",
    "# --- Interpolators ---\n",
    "# --- Create pchip interpolators (shape-preserving, no extrapolation) ---\n",
    "pchip_temp = PchipInterpolator(pressures, temperatures, extrapolate = False)\n",
    "pchip_sal = PchipInterpolator(pressures, salinities, extrapolate = False)\n",
    "# --- If profile does not start at 0 dbar, prepare linear extrapolator for shallow end ---\n",
    "min_p = pressures.min()\n",
    "if min_p > 0:\n",
    "# --- Use first two shallowest points for linear extrapolation to surface ---\n",
    " p_shallow = pressures[:2]\n",
    " t_shallow = temperatures[:2]\n",
    " s_shallow = salinities[:2]\n",
    " linear_temp = interp1d(p_shallow, t_shallow, fill_value=\"extrapolate\", kind=\"linear\")\n",
    " linear_sal = interp1d(p_shallow, s_shallow, fill_value=\"extrapolate\", kind=\"linear\")\n",
    "else:\n",
    " linear_temp = None\n",
    " linear_sal = None\n",
    "# --- Interpolate across grid ---\n",
    "# --- Initialize arrays with nans ---\n",
    "interp_temp = np.full_like(interp_depths, np.nan, dtype = float)\n",
    "interp_sal = np.full_like(interp_depths, np.nan, dtype = float)\n",
    "# --- Fill interpolation arrays ---\n",
    "for i, z in enumerate(interp_depths):\n",
    " if z < min_p:\n",
    "# --- Use linear extrapolation above observed range ---\n",
    " interp_temp[i] = linear_temp(z) if linear_temp else np.nan\n",
    " interp_sal[i] = linear_sal(z) if linear_sal else np.nan\n",
    " elif z <= pressures.max():\n",
    "# --- Use pchip interpolation within observed data range ---\n",
    " interp_temp[i] = pchip_temp(z)\n",
    " interp_sal[i] = pchip_sal(z)\n",
    "# --- Else: remains nan (deeper than profile) ---\n",
    "# --- Create final interpolated dataframe ---\n",
    "interp_profile = pd.DataFrame({\n",
    " 'pressure': interp_depths,\n",
    " 'temperature': interp_temp,\n",
    " 'salinity': interp_sal\n",
    "})\n",
    "# --- Save to csv ---\n",
    "interp_profile.to_csv('data/interp_profile.csv', index = False)\n",
    "# --- Plot temperature profile ---\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.gca().invert_yaxis() # So surface is at the top\n",
    "# --- Plot original points ---\n",
    "plt.scatter(filtered_profile['temperature'], filtered_profile['pressure'],\n",
    " label='Original Points', color='gray', s = 10, alpha = 0.6)\n",
    "# --- Plot clustered (deduplicated) points ---\n",
    "plt.scatter(cleaned_profile['temperature'], cleaned_profile['pressure'],\n",
    " label='Clustered Points', color='blue', s = 25)\n",
    "# --- Plot interpolated profile (pchip + linear) ---\n",
    "plt.plot(interp_profile['temperature'], interp_profile['pressure'],\n",
    " label='Interpolated (PCHIP + Linear)', color='red', linewidth = 2)\n",
    "# --- Labels and legend ---\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Pressure (dbar)')\n",
    "plt.title('Temperature Profile')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bcp)",
   "language": "python",
   "name": "bcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
