{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa790db6-e081-4e23-bced-683fd55e2581",
   "metadata": {},
   "source": [
    "<center><h2> Subsurface acoustics from biogeochemical floats as a pathway to scalable autonomous observations of global surface wind </h2></center>\n",
    "    \n",
    "<center><h3> Analysis and figures </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e282d-c582-4e60-86cc-6e1e2e7c8733",
   "metadata": {},
   "source": [
    "**L. Delaigue<sup>1</sup>\\, P. Cauchy<sup>2</sup>, D. Cazau<sup>3</sup>, R. Bozzano<sup>4</sup>, S. Pensieri<sup>4</sup>, A. Gros-Martial<sup>5</sup>, J. Bonnel<sup>6</sup>, E. Leymarie<sup>1</sup> and H. Claustre<sup>1</sup>**\n",
    "\n",
    "<sup>1</sup>Sorbonne Universit√©, CNRS, Laboratoire d'Oc√©anographie de Villefranche, LOV, 06230 Villefranche-sur-Mer, France\n",
    "\n",
    "<sup>2</sup>Institut des sciences de la mer (ISMER), Universit√© du Qu√©bec √† Rimouski (UQAR), Rimouski, Canada\n",
    "\n",
    "<sup>3</sup>ENSTA, Lab-STICC, UMR CNRS 6285, Brest, France\n",
    "\n",
    "<sup>4</sup>Institute for the Study of Anthropic Impact and Sustainability in the Marine Environment (IAS), Consiglio Nazionale delle Ricerche (CNR), Genoa, Italy\n",
    "\n",
    "<sup>5</sup>Centre d‚Äô√âtudes Biologiques de Chiz√©, CNRS, Villiers-en-bois, France\n",
    "\n",
    "<sup>6</sup>Marine Physical Laboratory, Scripps Institution of Oceanography, University of California San Diego, La Jolla, CA, 92093, USA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92246c38-7719-426d-a2dc-7cd88ffd40fb",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb80ff-d533-4e37-a562-8e4f15040ae2",
   "metadata": {},
   "source": [
    "Wind forcing plays a pivotal role in driving upper-ocean physical and biogeochemical processes, yet direct wind observations remain sparse in many regions of the global ocean. While passive acoustic techniques have been used to estimate wind speed from moored and mobile platforms, their application to profiling floats has been demonstrated only in limited cases and remains largely unexplored. Here, we report on the first deployment of a Biogeochemical-Argo (BGC-Argo) float equipped with a passive acoustic sensor, aimed at detecting wind-driven surface signals from depth. The float was deployed in the northwestern Mediterranean Sea near the DYFAMED meteorological buoy from February to April 2025, operating at parking depths of 500‚Äì1000‚ÄØm. We demonstrate that wind speed can be successfully retrieved from subsurface ambient noise using established acoustic algorithms, with float-derived estimates showing good agreement with collocated surface observations from the DYFAMED buoy. To evaluate the potential for broader application, we simulate a remote deployment scenario by refitting the acoustic model of Nystuen et al. (2015) using ERA5 reanalysis as a proxy for surface wind. Refitting the model to ERA5 data demonstrates that the float‚Äìacoustic‚Äìwind relationship is generalizable in moderate conditions, but high-wind regimes remain systematically biased‚Äîespecially above 10‚ÄØm‚ÄØs-1. Finally, we apply a residual learning framework to correct these estimates using a limited subset of DYFAMED wind data, simulating conditions where only brief surface observations‚Äîsuch as those from a ship during float deployment‚Äîare available. The corrected wind time series achieved a 37% reduction in RMSE and improved the coefficient of determination (R2) from 0.85 to 0.91, demonstrating the effectiveness of combining reanalysis with sparse in situ fitting. This framework enables the retrieval of fine-scale wind variability not captured by reanalysis alone, supporting a scalable strategy for float-based wind monitoring in data-sparse ocean regions‚Äîwith important implications for quantifying air‚Äìsea exchanges, improving biogeochemical flux estimates, and advancing global climate observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdc947-b81a-4c61-91f2-fac533646ff2",
   "metadata": {},
   "source": [
    "## What to expect from this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d4170-e8cc-402c-a32f-0f1e9c9ffc8c",
   "metadata": {},
   "source": [
    "### This notebook provides a complete walkthrough of the data processing and analysis workflows used in this study. It includes code for preprocessing float and reference datasets, computing acoustic metrics, applying and refitting empirical wind retrieval models, evaluating performance against in situ and reanalysis wind products, and generating all associated figures and metrics reported in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cbd0c-dc81-4951-a0f2-e7a5458d79cc",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7fe6f-3b21-4c17-a543-b63b6e4bc045",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e3dcd-178e-43e8-b7f2-f36e485f7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library Imports ===\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Scientific Computing & Data Handling ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    from scipy.integrate import cumulative_trapezoid as cumtrapz\n",
    "except ImportError:\n",
    "    from scipy.integrate import cumtrapz  # for older SciPy versions\n",
    "\n",
    "# === Plotting & Visualization ===\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# === Scientific Computation & Analysis ===\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# === Geospatial Calculations ===\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# === Carbonate System & Ocean Chemistry ===\n",
    "import PyCO2SYS as co2sys\n",
    "import pyseaflux\n",
    "\n",
    "# === Progress Bar for Iterations ===\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216fc66-b3f5-4464-9b89-86bd9be1d9d1",
   "metadata": {},
   "source": [
    "## 1 - Process raw float data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709a5ab-506f-4a2a-a4c3-e14935d05267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path pattern for all matching files\n",
    "file_pattern = \"data/float/csv/lovuse024c_*.csv\"  # Adjust the folder path if needed\n",
    "\n",
    "# Find all matching CSV files\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Define coordinate mapping (station_number -> (latitude, longitude))\n",
    "coordinate_map = {\n",
    "    # First deployment\n",
    "    \"040\": (43.698656, 7.308651),\n",
    "    \"041\": (43.395203, 7.867331),\n",
    "    \"042\": (43.385816, 7.835610),\n",
    "    \"043\": (43.383342, 7.805394),\n",
    "    \"044\": (43.374554, 7.780598),\n",
    "    \"045\": (43.347018, 7.761803),\n",
    "    \"046\": (43.318718, 7.743787),\n",
    "    \"047\": (43.306527, 7.715931),\n",
    "    \"048\": (43.293565, 7.683848),\n",
    "    \"049\": (43.271333, 7.634016),\n",
    "    \"050\": (43.250653, 7.578438),\n",
    "    \"051\": (43.237131, 7.537831),\n",
    "    \"052\": (43.221096, 7.488428),\n",
    "    \"053\": (43.181266, 7.432160),\n",
    "    \"054\": (43.143442, 7.408066),\n",
    "    \"055\": (43.113340, 7.377611),\n",
    "    \"056\": (43.094762, 7.322352),\n",
    "    \"057\": (43.080311, 7.263309),\n",
    "    \n",
    "    # Second deployment\n",
    "    \"058\": (43.417590, 7.798665),  \n",
    "    \"059\": (43.387746, 7.777208),\n",
    "    \"060\": (43.363777, 7.730860),\n",
    "    \"061\": (43.342675, 7.640585),\n",
    "    \"062\": (43.313547, 7.565492),\n",
    "    \"063\": (43.266750, 7.461486),\n",
    "    \"064\": (43.240656, 7.400188),\n",
    "    \"065\": (43.222551, 7.300435),\n",
    "    \"066\": (43.208608, 7.130404),\n",
    "    \"067\": (43.173637, 6.977106),\n",
    "    \"068\": (43.167145, 6.954846),\n",
    "    \"069\": (43.173564, 6.957003)\n",
    "}\n",
    "\n",
    "# List to store DataFrames\n",
    "float_data_list = []\n",
    "\n",
    "# Process each file\n",
    "for file in csv_files:\n",
    "    # Extract station number from filename\n",
    "    match = re.search(r'lovuse024c_(\\d{3})_', file)\n",
    "    if match:\n",
    "        station_number = match.group(1)  # Extract station number\n",
    "        if station_number in coordinate_map:\n",
    "            lat, lon = coordinate_map[station_number]\n",
    "\n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file, sep=\",\")\n",
    "            \n",
    "            # Add latitude, longitude, and station number columns\n",
    "            df[\"Latitude\"] = lat\n",
    "            df[\"Longitude\"] = lon\n",
    "            df[\"StationNumber\"] = station_number  # Store station number\n",
    "            \n",
    "            # Store in list\n",
    "            float_data_list.append(df)\n",
    "\n",
    "# Combine all files into one DataFrame\n",
    "float_data = pd.concat(float_data_list, ignore_index=True)\n",
    "\n",
    "# Convert Date column to datetime\n",
    "float_data[\"Date\"] = pd.to_datetime(float_data[\"Date\"])\n",
    "\n",
    "# Separate data based on SensorType\n",
    "sbe41_data = float_data[float_data[\"SensorType\"] == \"sbe41\"].copy()\n",
    "pal_data = float_data[float_data[\"SensorType\"] == \"pal\"].copy()\n",
    "do_data = float_data[float_data[\"SensorType\"] == \"do\"].copy()\n",
    "\n",
    "# Convert Date to matplotlib float format for KDTree\n",
    "sbe41_data[\"datenum\"] = sbe41_data[\"Date\"].map(mdates.date2num)\n",
    "pal_data[\"datenum\"] = pal_data[\"Date\"].map(mdates.date2num)\n",
    "do_data[\"datenum\"] = do_data[\"Date\"].map(mdates.date2num)\n",
    "\n",
    "# Ensure Pressure is numeric\n",
    "sbe41_data[\"Pressure_dbar\"] = pd.to_numeric(sbe41_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "pal_data[\"Pressure_dbar\"] = pd.to_numeric(pal_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "do_data[\"Pressure_dbar\"] = pd.to_numeric(do_data[\"Pressure_dbar\"], errors=\"coerce\")\n",
    "\n",
    "# DO remains the base dataset (7328 rows)\n",
    "final_data = do_data.copy()\n",
    "\n",
    "# Prepare empty lists for the new matched columns\n",
    "sbe41_matched_cols = {col: [] for col in sbe41_data.columns if col not in [\"datenum\", \"Pressure_dbar\", \"PhaseName\", \"SensorType\"]}\n",
    "pal_matched_cols = {col: [] for col in pal_data.columns if col not in [\"datenum\", \"Pressure_dbar\", \"PhaseName\", \"SensorType\"]}\n",
    "\n",
    "# Define the correction mapping based on frequency (Fr√©quence centrale de la BTO)\n",
    "# Define the correction mapping with Hz suffix to match column names\n",
    "frequency_correction = {\n",
    "    \"f_25000Hz\": -37.69, \"f_20000Hz\": -36.69, \"f_16000Hz\": -35.69, \"f_12500Hz\": -34.68, \n",
    "    \"f_10000Hz\": -33.68, \"f_8000Hz\": -32.68, \"f_6300Hz\": -31.67, \"f_5000Hz\": -30.67, \n",
    "    \"f_4000Hz\": -29.67, \"f_3150Hz\": -28.66, \"f_2500Hz\": -27.66, \"f_2000Hz\": -26.66, \n",
    "    \"f_1600Hz\": -25.65, \"f_1250Hz\": -24.65, \"f_1000Hz\": -23.65, \"f_800Hz\": -22.64, \n",
    "    \"f_630Hz\": -21.64, \"f_500Hz\": -20.64, \"f_400Hz\": -19.63, \"f_160Hz\": -15.62, \n",
    "    \"f_125Hz\": -14.62, \"f_100Hz\": -13.61, \"f_63Hz\": -11.61\n",
    "}\n",
    "\n",
    "# Initialize a list to store matched PAL timestamps\n",
    "matched_pal_dates = []\n",
    "\n",
    "# Iterate over each row in DO and find the nearest match in SBE41 & PAL\n",
    "print(\"Matching SBE41 and PAL data to DO dataset with 3-hour rolling mean...\")\n",
    "\n",
    "for _, do_row in tqdm(do_data.iterrows(), total=len(do_data)):\n",
    "    phase = do_row[\"PhaseName\"]\n",
    "    date_pressure = np.array([[do_row[\"datenum\"], do_row[\"Pressure_dbar\"]]])\n",
    "\n",
    "    # Find nearest SBE41 match (same PhaseName + nearest Date + nearest Pressure)\n",
    "    if not sbe41_data.empty and phase in sbe41_data[\"PhaseName\"].unique():\n",
    "        sbe41_phase = sbe41_data[sbe41_data[\"PhaseName\"] == phase]\n",
    "        if not sbe41_phase.empty:\n",
    "            tree_sbe41 = cKDTree(sbe41_phase[[\"datenum\", \"Pressure_dbar\"]].values)\n",
    "            _, idx_sbe41 = tree_sbe41.query(date_pressure)\n",
    "            matched_sbe41 = sbe41_phase.iloc[idx_sbe41[0]]\n",
    "        else:\n",
    "            matched_sbe41 = pd.Series(index=sbe41_data.columns, dtype=\"float64\")  # Empty row\n",
    "    else:\n",
    "        matched_sbe41 = pd.Series(index=sbe41_data.columns, dtype=\"float64\")  # Empty row\n",
    "\n",
    "    # Find nearest PAL match (same PhaseName + nearest Date + nearest Pressure)\n",
    "    if not pal_data.empty and phase in pal_data[\"PhaseName\"].unique():\n",
    "        pal_phase = pal_data[pal_data[\"PhaseName\"] == phase]\n",
    "        if not pal_phase.empty:\n",
    "            tree_pal = cKDTree(pal_phase[[\"datenum\", \"Pressure_dbar\"]].values)\n",
    "            _, idx_pal = tree_pal.query(date_pressure)\n",
    "            matched_pal = pal_phase.iloc[idx_pal[0]]  # Initial match\n",
    "            \n",
    "            # Store the timestamp before averaging\n",
    "            matched_pal_dates.append(matched_pal[\"Date\"])\n",
    "\n",
    "            # Define the rolling window: ¬±1.5 hours\n",
    "            time_window_start = matched_pal[\"Date\"] - timedelta(hours=1.5)\n",
    "            time_window_end = matched_pal[\"Date\"] + timedelta(hours=1.5)\n",
    "\n",
    "            # Select PAL rows within this time window\n",
    "            pal_subset = pal_phase[\n",
    "                (pal_phase[\"Date\"] >= time_window_start) & \n",
    "                (pal_phase[\"Date\"] <= time_window_end)\n",
    "            ].copy()\n",
    "\n",
    "            # Identify all f_* columns dynamically\n",
    "            f_columns = [col for col in pal_subset.columns if col.startswith(\"f_\")]\n",
    "\n",
    "            # Apply correction for each f_* column\n",
    "            for f_col in f_columns:\n",
    "                if f_col in frequency_correction:\n",
    "                    correction_value = frequency_correction[f_col]\n",
    "                    pal_subset[f_col] += correction_value\n",
    "                else:\n",
    "                    print(f\"Warning: No correction found for column {f_col}\")\n",
    "\n",
    "            # Flag PAL values above the 95th percentile before averaging\n",
    "            for f_col in f_columns:\n",
    "                threshold = pal_subset[f_col].quantile(0.95)\n",
    "                pal_subset.loc[pal_subset[f_col] > threshold, f_col] = np.nan\n",
    "\n",
    "            # Compute mean for numeric columns after applying correction\n",
    "            if not pal_subset.empty:\n",
    "                matched_pal = pal_subset.mean(numeric_only=True)\n",
    "            \n",
    "        else:\n",
    "            matched_pal = pd.Series(index=pal_data.columns, dtype=\"float64\")  # Empty row\n",
    "            matched_pal_dates.append(np.nan)  # Ensure we store NaN if no match found\n",
    "    else:\n",
    "        matched_pal = pd.Series(index=pal_data.columns, dtype=\"float64\")  # Empty row\n",
    "        matched_pal_dates.append(np.nan)  # Store NaN for missing timestamps\n",
    "\n",
    "    # Append matched data\n",
    "    for col in sbe41_matched_cols:\n",
    "        sbe41_matched_cols[col].append(matched_sbe41.get(col, np.nan))\n",
    "\n",
    "    for col in pal_matched_cols:\n",
    "        pal_matched_cols[col].append(matched_pal.get(col, np.nan))  # Store rolling mean\n",
    "\n",
    "# Add matched SBE41 and PAL columns to the final DataFrame\n",
    "for col, values in sbe41_matched_cols.items():\n",
    "    final_data[col + \"_sbe41\"] = values\n",
    "\n",
    "for col, values in pal_matched_cols.items():\n",
    "    final_data[col + \"_pal\"] = values\n",
    "\n",
    "# Add stored PAL timestamps\n",
    "final_data[\"datetime_pal\"] = matched_pal_dates\n",
    "\n",
    "# Convert datetime_pal to datetime format\n",
    "final_data[\"datetime_pal\"] = pd.to_datetime(final_data[\"datetime_pal\"], errors=\"coerce\")\n",
    "\n",
    "# Final check: Ensure DO row count remains the same as base\n",
    "assert len(final_data) == len(do_data), f\"Error: Final dataset has {len(final_data)} rows instead of {len(do_data)}!\"\n",
    "\n",
    "# Identify all columns that start with \"f_\" and end with \"_pal\"\n",
    "pal_f_columns = [col for col in final_data.columns if col.startswith(\"f_\") and col.endswith(\"_pal\")]\n",
    "\n",
    "# Define the list of other columns to keep\n",
    "cols_to_keep = [\n",
    "    \"StationNumber\",  # Include station number\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Pressure_dbar\",\n",
    "    \"Date\",\n",
    "    \"PhaseName\",\n",
    "    \"doxy_uncalibrated\",\n",
    "    \"datenum\",\n",
    "    \"Date_sbe41\",\n",
    "    \"Temperature_degC_sbe41\",\n",
    "    \"Salinity_PSU_sbe41\",\n",
    "    \"datetime_pal\",\n",
    "] + pal_f_columns  # Append dynamically identified PAL columns\n",
    "\n",
    "# Keep only the selected columns\n",
    "final_data = final_data[cols_to_keep]\n",
    "\n",
    "# Update renaming dictionary\n",
    "rename_dict = {\n",
    "    \"StationNumber\": \"station_number\",  # Rename station number\n",
    "    \"Latitude\":\"latitude\",\n",
    "    \"Longitude\":\"longitude\",\n",
    "    \"Pressure_dbar\": \"pressure\",\n",
    "    \"Date\": \"datetime_DO\",\n",
    "    \"PhaseName\": \"phasename\",\n",
    "    \"doxy_uncalibrated\": \"doxy\",\n",
    "    \"datenum\": \"datenum\",\n",
    "    \"Date_sbe41\": \"datetime_SBE41\",\n",
    "    \"Temperature_degC_sbe41\": \"temperature\",\n",
    "    \"Salinity_PSU_sbe41\": \"salinity\",\n",
    "}\n",
    "\n",
    "# Add dynamically identified PAL renaming\n",
    "rename_dict.update({col: col.replace(\"_pal\", \"\") for col in pal_f_columns})\n",
    "\n",
    "float_data = final_data.rename(columns=rename_dict)\n",
    "del final_data\n",
    "\n",
    "# Converting 'datetime_DO' to datetime format if it's not already\n",
    "float_data['datetime_DO'] = pd.to_datetime(float_data['datetime_DO'], errors='coerce')\n",
    "\n",
    "# final_data new columns for year, month, and day\n",
    "float_data['year'] = float_data['datetime_DO'].dt.year\n",
    "float_data['month'] = float_data['datetime_DO'].dt.month\n",
    "float_data['day'] = float_data['datetime_DO'].dt.day\n",
    "\n",
    "# Convert datetime to numerical format for plotting later\n",
    "float_data[\"datenum\"] = mdates.date2num(float_data[\"datetime_DO\"])\n",
    "\n",
    "# Ensuite station number remains\n",
    "float_data[\"station_number\"] = float_data[\"station_number\"].astype(str)  # Ensure it's treated as a string\n",
    "\n",
    "# Save final merged dataset\n",
    "float_data.to_csv(\"data/sea_trials_do_sbe41_pal.csv\", index=False)\n",
    "\n",
    "print(\"Merged data saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59398c6-c235-4567-9632-2dcbcc888c1c",
   "metadata": {},
   "source": [
    "## 2 - Normalize TOL to dB re 1¬µPa2/Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84c2d6-089f-4699-b25b-3594214d74dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4668bb-7687-4d38-bb01-605acd0c3cca",
   "metadata": {},
   "source": [
    "## 3 - Process DYFAMED meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049f241-73f0-4a1d-b7b7-933f1edcba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both months' data and combine\n",
    "files = [\"data/meteo_france_vent/marine.202502.csv\", \"data/meteo_france_vent/marine.202503.csv\", \"data/meteo_france_vent/marine.202504.csv\"]\n",
    "wind_data = pd.concat([pd.read_csv(f, sep=\";\", na_values='mq') for f in files], ignore_index=True)\n",
    "\n",
    "# Only keep data for DYFAMED\n",
    "wind_data = wind_data[wind_data['numer_sta'] == '6100001']\n",
    "\n",
    "# Rename columns\n",
    "wind_data = wind_data.rename(columns={\n",
    "    \"date\": \"datetime\",\n",
    "    \"ff\": \"U_DYFAMED\",\n",
    "})[[\"datetime\", \"U_DYFAMED\"]]\n",
    "\n",
    "# Function to parse datetime in \"YYYYMMDDHHMMSS\" format\n",
    "def parse_datetime(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(str(date_str), \"%Y%m%d%H%M%S\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date: {date_str} - {e}\")\n",
    "        return np.nan  # Return NaN if parsing fails\n",
    "\n",
    "# Apply datetime parsing\n",
    "wind_data[\"datetime\"] = wind_data[\"datetime\"].apply(parse_datetime)\n",
    "\n",
    "# Drop any rows where datetime parsing failed\n",
    "wind_data = wind_data.dropna(subset=[\"datetime\"])\n",
    "\n",
    "# Convert datetime columns to datetime format\n",
    "# float_data[\"Date\"] = pd.to_datetime(float_data[\"Date\"])\n",
    "wind_data[\"datetime\"] = pd.to_datetime(wind_data[\"datetime\"])\n",
    "\n",
    "# Ensure wind_data is sorted and has unique timestamps\n",
    "wind_data = wind_data.sort_values(\"datetime\").drop_duplicates(subset=[\"datetime\"])\n",
    "\n",
    "# Convert datetime to numerical format for plotting\n",
    "wind_data[\"datenum\"] = mdates.date2num(wind_data[\"datetime\"])\n",
    "\n",
    "# Add latitude and longitude\n",
    "wind_data[\"lat\"] = 43.38\n",
    "wind_data[\"lon\"] = 7.83\n",
    "\n",
    "wind_data.to_csv(\"data/wind_data_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810e950-a15f-49a9-b245-f8133c85c70c",
   "metadata": {},
   "source": [
    "## 4 - Match DYFAMED wind to float data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45046d1d-a7b5-4cb0-9d1b-10a6107f9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns to datetime format\n",
    "float_data = float_data.reset_index(drop=True)\n",
    "float_data[\"datetime_DO\"] = pd.to_datetime(float_data[\"datetime_DO\"])\n",
    "wind_data[\"datetime\"] = pd.to_datetime(wind_data[\"datetime\"])\n",
    "\n",
    "# Ensure wind_data is sorted and has unique timestamps\n",
    "wind_data = wind_data.sort_values(\"datetime\").drop_duplicates(subset=[\"datetime\"])\n",
    "\n",
    "# Match with wind data using the nearest timestamp approach\n",
    "matched_wind_data = wind_data.set_index(\"datetime\").reindex(float_data[\"datetime_DO\"], method=\"nearest\").reset_index()\n",
    "\n",
    "# Add matched wind speed directly to float_data\n",
    "float_data[\"U_DYFAMED\"] = matched_wind_data[\"U_DYFAMED\"]\n",
    "\n",
    "# Extract matched f_8000Hz and wind speed (after ensuring non-null values)\n",
    "X_data = float_data[\"f_8000Hz\"].dropna()\n",
    "Y_data = float_data[\"U_DYFAMED\"].dropna()\n",
    "\n",
    "# Ensure both datasets align (remove unmatched indices)\n",
    "common_indices = X_data.index.intersection(Y_data.index)\n",
    "X_data = X_data.loc[common_indices]\n",
    "Y_data = Y_data.loc[common_indices]\n",
    "\n",
    "# Define DYFAMED buoy coordinates\n",
    "buoy_lat, buoy_lon = wind_data[\"lat\"].unique(), wind_data[\"lon\"].unique()\n",
    "\n",
    "# Ensure 'Latitude' and 'Longitude' exist and do not contain NaN values\n",
    "if \"latitude\" in float_data.columns and \"longitude\" in float_data.columns:\n",
    "    # Remove rows with NaN in Latitude or Longitude before applying distance function\n",
    "    valid_data = float_data.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "\n",
    "    # Apply geodesic distance calculation only for valid rows\n",
    "    valid_data[\"distance_km\"] = valid_data.apply(\n",
    "        lambda row: geodesic((row[\"latitude\"], row[\"longitude\"]), (buoy_lat, buoy_lon)).km, axis=1\n",
    "    )\n",
    "\n",
    "    # Merge back with float_data (ensure NaN distances remain for missing coordinates)\n",
    "    float_data = float_data.merge(valid_data[[\"datetime_DO\", \"distance_km\"]], on=\"datetime_DO\", how=\"left\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: 'latitude' and 'longitude' columns not found in float_data.\")\n",
    "\n",
    "# Normalize distances (only if distance_km exists and is valid)\n",
    "if \"distance_km\" in float_data.columns:\n",
    "    float_data[\"norm_distance\"] = (float_data[\"distance_km\"] - float_data[\"distance_km\"].min()) / \\\n",
    "                                  (float_data[\"distance_km\"].max() - float_data[\"distance_km\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c564dd-679b-4de8-8cb9-b990fd68558e",
   "metadata": {},
   "source": [
    "## 5 - Apply depth correction based on Cauchy et al. (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4805eba-e078-4687-b3d9-3032dbb1622b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9ed3d-641b-49dd-a47f-e8350e05d519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943c2bc-7318-449c-82bd-d0e6632af4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df19109-4c3f-4b31-9118-0684c2bc5410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c10c1d-14ec-44a3-9601-24c9dba3835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744446f-37aa-4d98-bdf2-d2e575df7951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23e1d41-b830-48eb-b9fa-d1b0f33c894c",
   "metadata": {},
   "source": [
    "## X - Process AIS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5a928-f1dc-4064-a89f-981947ec7d71",
   "metadata": {},
   "source": [
    "### Parse AIS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5224ec4-2e54-490b-8d1e-af06880f7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gzip\n",
    "# import pandas as pd\n",
    "# import ais.stream\n",
    "# import re\n",
    "\n",
    "# # --- Config ---\n",
    "# LIMIT_FILES = None  # Set to an integer to limit how many files are processed\n",
    "\n",
    "# base_dir = r\"C:\\Users\\louis\\Documents\\GitHub\\tricuso-init\\data\\AIS\"\n",
    "# output_dir = os.path.join(base_dir, \"parsed\")\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # --- Vessel type lookup ---\n",
    "# type_lookup = {\n",
    "#     0: 'Not available',\n",
    "#     **{k: 'Reserved' for k in range(1, 20)},\n",
    "#     20: 'WIG', 21: 'WIG - Hazardous A', 22: 'WIG - Hazardous B', 23: 'WIG - Hazardous C', 24: 'WIG - Hazardous D',\n",
    "#     **{k: 'WIG - Reserved' for k in range(25, 30)},\n",
    "#     30: 'Fishing', 31: 'Towing', 32: 'Towing - long', 33: 'Dredging or underwater ops', 34: 'Diving ops',\n",
    "#     35: 'Military ops', 36: 'Sailing', 37: 'Pleasure craft', 38: 'Reserved', 39: 'Reserved',\n",
    "#     40: 'HSC', 41: 'HSC - Hazardous A', 42: 'HSC - Hazardous B', 43: 'HSC - Hazardous C', 44: 'HSC - Hazardous D',\n",
    "#     **{k: 'HSC - Reserved' for k in range(45, 49)}, 49: 'HSC - No additional info',\n",
    "#     50: 'Pilot vessel', 51: 'Search and rescue', 52: 'Tug', 53: 'Port tender',\n",
    "#     54: 'Anti-pollution', 55: 'Law enforcement', 56: 'Local vessel (spare)', 57: 'Local vessel (spare)',\n",
    "#     58: 'Medical transport', 59: 'Noncombatant under RR Res. No. 18',\n",
    "#     60: 'Passenger', 61: 'Passenger - Hazardous A', 62: 'Passenger - Hazardous B',\n",
    "#     63: 'Passenger - Hazardous C', 64: 'Passenger - Hazardous D',\n",
    "#     **{k: 'Passenger - Reserved' for k in range(65, 69)}, 69: 'Passenger - No additional info',\n",
    "#     70: 'Cargo', 71: 'Cargo - Hazardous A', 72: 'Cargo - Hazardous B', 73: 'Cargo - Hazardous C', 74: 'Cargo - Hazardous D',\n",
    "#     **{k: 'Cargo - Reserved' for k in range(75, 79)}, 79: 'Cargo - No additional info',\n",
    "#     80: 'Tanker', 81: 'Tanker - Hazardous A', 82: 'Tanker - Hazardous B',\n",
    "#     83: 'Tanker - Hazardous C', 84: 'Tanker - Hazardous D',\n",
    "#     **{k: 'Tanker - Reserved' for k in range(85, 89)}, 89: 'Tanker - No additional info',\n",
    "#     90: 'Other', 91: 'Other - Hazardous A', 92: 'Other - Hazardous B', 93: 'Other - Hazardous C',\n",
    "#     94: 'Other - Hazardous D', **{k: 'Other - Reserved' for k in range(95, 99)}, 99: 'Other - No additional info'\n",
    "# }\n",
    "\n",
    "# def decode_type(type_code):\n",
    "#     if pd.isna(type_code):\n",
    "#         return None\n",
    "#     try:\n",
    "#         return type_lookup.get(int(type_code), 'Unknown')\n",
    "#     except:\n",
    "#         return 'Unknown'\n",
    "\n",
    "# # --- File Discovery ---\n",
    "# files = sorted([f for f in os.listdir(base_dir) if f.startswith(\"messages-\")])\n",
    "# if LIMIT_FILES is not None:\n",
    "#     files = files[:LIMIT_FILES]\n",
    "\n",
    "# # --- Main Loop ---\n",
    "# for filename in files:\n",
    "#     if not filename.startswith(\"messages-\"):\n",
    "#         continue\n",
    "\n",
    "#     # Extract date from filename\n",
    "#     match = re.match(r\"messages-(\\d{4})(\\d{2})(\\d{2})(?:\\.gz)?$\", filename)\n",
    "#     if not match:\n",
    "#         print(f\"‚ö†Ô∏è Skipping file with unexpected format: {filename}\")\n",
    "#         continue\n",
    "#     year, month, day = map(int, match.groups())\n",
    "\n",
    "#     filepath = os.path.join(base_dir, filename)\n",
    "#     opener = gzip.open if filename.endswith(\".gz\") else open\n",
    "\n",
    "#     messages = []\n",
    "#     print(f\"üì¶ Processing {filename}\")\n",
    "\n",
    "#     try:\n",
    "#         with opener(filepath, mode='rt', encoding='utf-8', errors='ignore') as f:\n",
    "#             for msg in ais.stream.decode(f):\n",
    "#                 if isinstance(msg, dict):\n",
    "#                     messages.append(msg)\n",
    "\n",
    "#         # Convert to DataFrame\n",
    "#         df_chunk = pd.DataFrame(messages)\n",
    "\n",
    "#         # Drop rows without valid lat/lon\n",
    "#         df_chunk = df_chunk.dropna(subset=['x', 'y'])\n",
    "\n",
    "#         # Add vessel type\n",
    "#         df_chunk['vessel_type'] = df_chunk['type_and_cargo'].apply(decode_type)\n",
    "\n",
    "#         # Add file date info\n",
    "#         df_chunk['file_year'] = year\n",
    "#         df_chunk['file_month'] = month\n",
    "#         df_chunk['file_day'] = day\n",
    "\n",
    "#         # Save to unique file\n",
    "#         output_filename = f\"ais_positions_{year:04d}{month:02d}{day:02d}.csv\"\n",
    "#         output_path = os.path.join(output_dir, output_filename)\n",
    "#         df_chunk.to_csv(output_path, index=False)\n",
    "\n",
    "#         print(f\"‚úÖ Saved {len(df_chunk)} rows to {output_filename}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to process {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d1865-1f17-42fa-af8a-81048c075622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c9da9-4ff8-4ce1-bebd-a676dff535ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Geod\n",
    "from datetime import timedelta\n",
    "from haversine import haversine, Unit\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Load float data and parse datetimes\n",
    "float_data = pd.read_csv(\"data/sea_trials_do_sbe41_pal.csv\")\n",
    "float_data['datetime_DO'] = pd.to_datetime(float_data['datetime_DO'], errors='coerce')\n",
    "float_data['nearby_ships'] = np.nan  # prefill with NaN\n",
    "\n",
    "# Interpolate float trajectory\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "stations_df = float_data[[\"station_number\", \"latitude\", \"longitude\", \"datetime_DO\"]].drop_duplicates(\n",
    "    subset=[\"station_number\", \"latitude\", \"longitude\"]\n",
    ").sort_values(\"datetime_DO\").reset_index(drop=True)\n",
    "\n",
    "interpolated_rows = []\n",
    "for i in range(len(stations_df) - 1):\n",
    "    start = stations_df.iloc[i]\n",
    "    end = stations_df.iloc[i + 1]\n",
    "    start_time = pd.to_datetime(start[\"datetime_DO\"])\n",
    "    end_time = pd.to_datetime(end[\"datetime_DO\"])\n",
    "    duration_hours = int((end_time - start_time).total_seconds() // 3600)\n",
    "    if duration_hours <= 0:\n",
    "        continue\n",
    "    lonlats = geod.npts(start[\"longitude\"], start[\"latitude\"], end[\"longitude\"], end[\"latitude\"], duration_hours - 1)\n",
    "    interpolated_rows.append({\"datetime\": start_time, \"latitude_interp\": start[\"latitude\"], \"longitude_interp\": start[\"longitude\"]})\n",
    "    for j, (lon, lat) in enumerate(lonlats):\n",
    "        interpolated_rows.append({\"datetime\": start_time + timedelta(hours=j + 1), \"latitude_interp\": lat, \"longitude_interp\": lon})\n",
    "interpolated_rows.append({\n",
    "    \"datetime\": stations_df.iloc[-1][\"datetime_DO\"],\n",
    "    \"latitude_interp\": stations_df.iloc[-1][\"latitude\"],\n",
    "    \"longitude_interp\": stations_df.iloc[-1][\"longitude\"]\n",
    "})\n",
    "interp_df = pd.DataFrame(interpolated_rows)\n",
    "interp_df[\"datetime\"] = pd.to_datetime(interp_df[\"datetime\"])\n",
    "\n",
    "# Merge interpolated trajectory with float_data\n",
    "float_data = pd.merge_asof(\n",
    "    float_data.sort_values(\"datetime_DO\"),\n",
    "    interp_df.sort_values(\"datetime\"),\n",
    "    left_on=\"datetime_DO\",\n",
    "    right_on=\"datetime\",\n",
    "    direction=\"nearest\"\n",
    ")\n",
    "\n",
    "# Loop through AIS files\n",
    "radius_km = 20\n",
    "time_window_minutes = 30\n",
    "data_folder = \"data/AIS/parsed/\"\n",
    "bounding_box = (6.5, 8.0, 43.0, 44.0)\n",
    "\n",
    "for filepath in glob(os.path.join(data_folder, \"ais_positions_*.csv\")):\n",
    "    print(f\"Processing {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        ais = pd.read_csv(filepath, on_bad_lines='skip', low_memory=False)\n",
    "        ais = ais.dropna(subset=['utc_hour', 'utc_min', 'x', 'y'])\n",
    "\n",
    "        west, east, south, north = bounding_box\n",
    "        ais = ais[\n",
    "            (ais['x'] >= west) & (ais['x'] <= east) &\n",
    "            (ais['y'] >= south) & (ais['y'] <= north)\n",
    "        ]\n",
    "\n",
    "        ais['datetime'] = pd.to_datetime(dict(\n",
    "            year=ais['file_year'].astype(int),\n",
    "            month=ais['file_month'].astype(int),\n",
    "            day=ais['file_day'].astype(int),\n",
    "            hour=ais['utc_hour'].astype(int),\n",
    "            minute=ais['utc_min'].astype(int)\n",
    "        ), errors='coerce')\n",
    "\n",
    "        if ais.empty:\n",
    "            continue\n",
    "\n",
    "        # Determine the date of this AIS file\n",
    "        ais_date = ais['datetime'].dt.date.unique()[0]\n",
    "        float_mask = float_data['datetime_DO'].dt.date == ais_date\n",
    "\n",
    "        if not float_mask.any():\n",
    "            continue\n",
    "\n",
    "        # Subset float_data to rows for this date\n",
    "        float_subset = float_data[float_mask].copy()\n",
    "        ship_counts = []\n",
    "\n",
    "        for _, float_row in float_subset.iterrows():\n",
    "            lat_f = float_row['latitude_interp']\n",
    "            lon_f = float_row['longitude_interp']\n",
    "            time_f = float_row['datetime_DO']\n",
    "\n",
    "            if pd.isna(time_f) or pd.isna(lat_f) or pd.isna(lon_f):\n",
    "                ship_counts.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            start = time_f - pd.Timedelta(minutes=time_window_minutes)\n",
    "            end = time_f + pd.Timedelta(minutes=time_window_minutes)\n",
    "\n",
    "            ais_window = ais[(ais['datetime'] >= start) & (ais['datetime'] <= end)]\n",
    "            ais_dedup = ais_window.sort_values('datetime').drop_duplicates(subset='mmsi', keep='first')\n",
    "\n",
    "            distances = ais_dedup.apply(\n",
    "                lambda row: haversine((lat_f, lon_f), (row['y'], row['x']), unit=Unit.KILOMETERS),\n",
    "                axis=1\n",
    "            )\n",
    "            ship_counts.append((distances <= radius_km).sum())\n",
    "\n",
    "        # Insert results for that day only\n",
    "        float_data.loc[float_mask, 'nearby_ships'] = ship_counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {filepath}: {e}\")\n",
    "        continue\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Compute diff\n",
    "float_data['U_diff'] = float_data['U_nystuen_opt_coeffs'] - float_data['U_DYFAMED']\n",
    "\n",
    "# Ensure datetime format and sorting\n",
    "float_data = float_data.sort_values('datetime')\n",
    "float_data['datetime'] = pd.to_datetime(float_data['datetime'])\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = (float_data['U_diff']**2).mean()**0.5\n",
    "\n",
    "# Find ship times\n",
    "ship_times = float_data[float_data['nearby_ships'] > 0]['datetime']\n",
    "\n",
    "# Define \"significant deviation + ship nearby\"\n",
    "def highlight_condition(dt, diff):\n",
    "    is_significant = abs(diff) > rmse\n",
    "    is_ship_nearby = any(abs((dt - ship_time).total_seconds()) <= 1800 for ship_time in ship_times)\n",
    "    return is_significant and is_ship_nearby\n",
    "\n",
    "float_data['highlight'] = float_data.apply(\n",
    "    lambda row: highlight_condition(row['datetime'], row['U_diff']), axis=1\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Segment-wise coloring\n",
    "current_color = None\n",
    "segment_x, segment_y = [], []\n",
    "\n",
    "for idx, row in float_data.iterrows():\n",
    "    color = 'red' if row['highlight'] else 'tab:blue'\n",
    "    if current_color is None:\n",
    "        current_color = color\n",
    "\n",
    "    if color != current_color and segment_x:\n",
    "        ax.plot(segment_x, segment_y, color=current_color)\n",
    "        segment_x, segment_y = [], []\n",
    "        current_color = color\n",
    "\n",
    "    segment_x.append(row['datetime'])\n",
    "    segment_y.append(row['U_diff'])\n",
    "\n",
    "# Final segment\n",
    "if segment_x:\n",
    "    ax.plot(segment_x, segment_y, color=current_color)\n",
    "\n",
    "# Labels and layout\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('U Difference (m/s)')\n",
    "ax.set_title(f'Deviation U Float - U DYFAMED (Red = |Diff| > RMSE ‚âà {rmse:.2f} & Ship Nearby)')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "float_data = float_data[~float_data['highlight']]\n",
    "\n",
    "float_data.to_csv(\"data/sea_trials_do_sbe41_pal_pco2_depth_corrected_wind_opt_fits_ERA5_postAIS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354cadf-706e-4ce8-9ae5-239c968d183f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b90912-5006-4817-9a02-50a385edf004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bcp)",
   "language": "python",
   "name": "bcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
